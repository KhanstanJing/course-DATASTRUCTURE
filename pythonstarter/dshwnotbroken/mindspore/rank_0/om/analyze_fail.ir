# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
For 'MatMul' the input dimensions must be equal, but got 'x1_col': 12800 and 'x2_row': 6400.

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\mat_mul.cc:107 mindspore::ops::MatMulInfer::InferShape

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417
        if not self.sense_flag:
# 1 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418
            return self._no_sens_impl(*inputs)
                   ^
# 2 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437
        if self.return_grad:
# 3 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433
        loss = self.network(*inputs)
               ^
# 4 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121
        out = self._backbone(data)
              ^
# 5 In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:100
        x = self.fc1(x)
            ^
# 6 In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:106
        x = self.fc2(x)
            ^
# 7 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:628
        if self.has_bias:
# 8 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:629
            x = self.bias_add(x, self.bias)
            ^
# 9 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:630
        if self.activation_flag:
# 10 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632
        if len(x_shape) != 2:
# 11 In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:627
        x = self.matmul(x, self.weight)
            ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12
# Total subgraphs: 250

# Attrs:
training : 1

# Total params: 62
# Params:
%para1_inputs0 : <null>
%para2_inputs1 : <null>
%para3_conv1.weight : <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:conv1.weight>  :  has_default
%para4_bn1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>  :  has_default
%para5_bn1.beta : <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>  :  has_default
%para6_conv2.weight : <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:conv2.weight>  :  has_default
%para7_bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:bn2.gamma>  :  has_default
%para8_bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:bn2.beta>  :  has_default
%para9_conv3.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:conv3.weight>  :  has_default
%para10_bn3.gamma : <Ref[Tensor[Float32]], (64), ref_key=:bn3.gamma>  :  has_default
%para11_bn3.beta : <Ref[Tensor[Float32]], (64), ref_key=:bn3.beta>  :  has_default
%para12_conv4.weight : <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:conv4.weight>  :  has_default
%para13_bn4.gamma : <Ref[Tensor[Float32]], (128), ref_key=:bn4.gamma>  :  has_default
%para14_bn4.beta : <Ref[Tensor[Float32]], (128), ref_key=:bn4.beta>  :  has_default
%para15_fc1.weight : <Ref[Tensor[Float32]], (128, 6400), ref_key=:fc1.weight>  :  has_default
%para16_fc1.bias : <Ref[Tensor[Float32]], (128), ref_key=:fc1.bias>  :  has_default
%para17_fc2.weight : <Ref[Tensor[Float32]], (3, 128), ref_key=:fc2.weight>  :  has_default
%para18_fc2.bias : <Ref[Tensor[Float32]], (3), ref_key=:fc2.bias>  :  has_default
%para19_moment1.conv1.weight : <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment1.conv1.weight>  :  has_default
%para20_moment1.bn1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn1.gamma>  :  has_default
%para21_moment1.bn1.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn1.beta>  :  has_default
%para22_moment1.conv2.weight : <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:moment1.conv2.weight>  :  has_default
%para23_moment1.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn2.gamma>  :  has_default
%para24_moment1.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn2.beta>  :  has_default
%para25_moment1.conv3.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moment1.conv3.weight>  :  has_default
%para26_moment1.bn3.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn3.gamma>  :  has_default
%para27_moment1.bn3.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn3.beta>  :  has_default
%para28_moment1.conv4.weight : <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:moment1.conv4.weight>  :  has_default
%para29_moment1.bn4.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment1.bn4.gamma>  :  has_default
%para30_moment1.bn4.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment1.bn4.beta>  :  has_default
%para31_moment1.fc1.weight : <Ref[Tensor[Float32]], (128, 6400), ref_key=:moment1.fc1.weight>  :  has_default
%para32_moment1.fc1.bias : <Ref[Tensor[Float32]], (128), ref_key=:moment1.fc1.bias>  :  has_default
%para33_moment1.fc2.weight : <Ref[Tensor[Float32]], (3, 128), ref_key=:moment1.fc2.weight>  :  has_default
%para34_moment1.fc2.bias : <Ref[Tensor[Float32]], (3), ref_key=:moment1.fc2.bias>  :  has_default
%para35_moment2.conv1.weight : <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment2.conv1.weight>  :  has_default
%para36_moment2.bn1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn1.gamma>  :  has_default
%para37_moment2.bn1.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn1.beta>  :  has_default
%para38_moment2.conv2.weight : <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:moment2.conv2.weight>  :  has_default
%para39_moment2.bn2.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn2.gamma>  :  has_default
%para40_moment2.bn2.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn2.beta>  :  has_default
%para41_moment2.conv3.weight : <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moment2.conv3.weight>  :  has_default
%para42_moment2.bn3.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn3.gamma>  :  has_default
%para43_moment2.bn3.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn3.beta>  :  has_default
%para44_moment2.conv4.weight : <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:moment2.conv4.weight>  :  has_default
%para45_moment2.bn4.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment2.bn4.gamma>  :  has_default
%para46_moment2.bn4.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment2.bn4.beta>  :  has_default
%para47_moment2.fc1.weight : <Ref[Tensor[Float32]], (128, 6400), ref_key=:moment2.fc1.weight>  :  has_default
%para48_moment2.fc1.bias : <Ref[Tensor[Float32]], (128), ref_key=:moment2.fc1.bias>  :  has_default
%para49_moment2.fc2.weight : <Ref[Tensor[Float32]], (3, 128), ref_key=:moment2.fc2.weight>  :  has_default
%para50_moment2.fc2.bias : <Ref[Tensor[Float32]], (3), ref_key=:moment2.fc2.bias>  :  has_default
%para51_beta1_power : <Ref[Tensor[Float32]], (), ref_key=:beta1_power>  :  has_default
%para52_beta2_power : <Ref[Tensor[Float32]], (), ref_key=:beta2_power>  :  has_default
%para53_global_step : <Ref[Tensor[Int32]], (1), ref_key=:global_step>  :  has_default
%para54_learning_rate : <Ref[Tensor[Float32]], (), ref_key=:learning_rate>  :  has_default
%para55_bn4.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:bn4.moving_mean>  :  has_default
%para56_bn4.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:bn4.moving_variance>  :  has_default
%para57_bn3.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:bn3.moving_mean>  :  has_default
%para58_bn3.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:bn3.moving_variance>  :  has_default
%para59_bn2.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:bn2.moving_mean>  :  has_default
%para60_bn2.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:bn2.moving_variance>  :  has_default
%para61_bn1.moving_mean : <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_mean>  :  has_default
%para62_bn1.moving_variance : <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_variance>  :  has_default

subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12 : 0000028E6F1165F0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12(%para1_inputs0, %para2_inputs1, %para3_conv1.weight, %para4_bn1.gamma, %para5_bn1.beta, %para6_conv2.weight, %para7_bn2.gamma, %para8_bn2.beta, %para9_conv3.weight, %para10_bn3.gamma, %para11_bn3.beta, %para12_conv4.weight, %para13_bn4.gamma, %para14_bn4.beta, %para15_fc1.weight, %para16_fc1.bias, %para17_fc2.weight, %para18_fc2.bias, %para19_moment1.conv1.weight, %para20_moment1.bn1.gamma, %para21_moment1.bn1.beta, %para22_moment1.conv2.weight, %para23_moment1.bn2.gamma, %para24_moment1.bn2.beta, %para25_moment1.conv3.weight, %para26_moment1.bn3.gamma, %para27_moment1.bn3.beta, %para28_moment1.conv4.weight, %para29_moment1.bn4.gamma, %para30_moment1.bn4.beta, %para31_moment1.fc1.weight, %para32_moment1.fc1.bias, %para33_moment1.fc2.weight, %para34_moment1.fc2.bias, %para35_moment2.conv1.weight, %para36_moment2.bn1.gamma, %para37_moment2.bn1.beta, %para38_moment2.conv2.weight, %para39_moment2.bn2.gamma, %para40_moment2.bn2.beta, %para41_moment2.conv3.weight, %para42_moment2.bn3.gamma, %para43_moment2.bn3.beta, %para44_moment2.conv4.weight, %para45_moment2.bn4.gamma, %para46_moment2.bn4.beta, %para47_moment2.fc1.weight, %para48_moment2.fc1.bias, %para49_moment2.fc2.weight, %para50_moment2.fc2.bias, %para51_beta1_power, %para52_beta2_power, %para53_global_step, %para54_learning_rate, %para55_bn4.moving_mean, %para56_bn4.moving_variance, %para57_bn3.moving_mean, %para58_bn3.moving_variance, %para59_bn2.moving_mean, %para60_bn2.moving_variance, %para61_bn1.moving_mean, %para62_bn1.moving_variance) {

#------------------------> 0
  %1(CNode_33) = call @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13()
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12:CNode_33{[0]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12:CNode_34{[0]: ValueNode<Primitive> Return, [1]: CNode_33}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13 : 0000028E6F115600
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12]() {
  %1(CNode_35) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12):MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[Float32], (1, 1, 150, 150)>, <Tensor[Int32], (1)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/

#------------------------> 1
  %2(CNode_36) = UnpackCall_unpack_call(@_no_sens_impl_37, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13:CNode_36{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.38, [1]: ValueNode<FuncGraph> _no_sens_impl_37, [2]: CNode_35}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13:CNode_39{[0]: ValueNode<Primitive> Return, [1]: CNode_36}


subgraph attr:
core : 1
subgraph instance: UnpackCall_14 : 0000028E6F353F50
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
subgraph @UnpackCall_14(%para63_, %para64_) {
  %1(CNode_36) = TupleGetItem(%para64_16, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>, <Int64, NoShape>) -> (<Tensor[Float32], (1, 1, 150, 150)>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  %2(CNode_36) = TupleGetItem(%para64_16, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>, <Int64, NoShape>) -> (<Tensor[Int32], (1)>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/

#------------------------> 2
  %3(CNode_36) = %para63_15(%1, %2)
      : (<Tensor[Float32], (1, 1, 150, 150)>, <Tensor[Int32], (1)>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @UnpackCall_14:CNode_36{[0]: param_15, [1]: CNode_36, [2]: CNode_36}
#   2: @UnpackCall_14:CNode_36{[0]: ValueNode<Primitive> Return, [1]: CNode_36}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_17 : 0000028E6F3549F0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_17 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para65_inputs0, %para66_inputs1) {

#------------------------> 3
  %1(CNode_40) = call @_no_sens_impl_18()
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_17:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.41, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23, [2]: CNode_42}
#   2: @_no_sens_impl_17:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23, [2]: CNode_42}
#   3: @_no_sens_impl_17:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_43}
#   4: @_no_sens_impl_17:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.44, [1]: grads, [2]: CNode_42}
#   5: @_no_sens_impl_17:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_45, [1]: grads}
#   6: @_no_sens_impl_17:CNode_46{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_47, [1]: grads}
#   7: @_no_sens_impl_17:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_46}
#   8: @_no_sens_impl_17:CNode_40{[0]: ValueNode<FuncGraph> _no_sens_impl_18}
#   9: @_no_sens_impl_17:CNode_48{[0]: ValueNode<Primitive> Return, [1]: CNode_40}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_18 : 0000028E6F34E500
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_18 parent: [subgraph @_no_sens_impl_17]() {

#------------------------> 4
  %1(CNode_49) = call @_no_sens_impl_19()
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_18:CNode_49{[0]: ValueNode<FuncGraph> _no_sens_impl_19}
#   2: @_no_sens_impl_18:CNode_50{[0]: ValueNode<Primitive> Return, [1]: CNode_49}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_19 : 0000028E6F354F40
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_19 parent: [subgraph @_no_sens_impl_17]() {
  %1(CNode_42) = $(_no_sens_impl_17):MakeTuple(%para65_inputs0, %para66_inputs1)
      : (<Tensor[Float32], (1, 1, 150, 150)>, <Tensor[Int32], (1)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/

#------------------------> 5
  %2(loss) = $(_no_sens_impl_17):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %3(grads) = $(_no_sens_impl_17):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(CNode_43) = $(_no_sens_impl_17):MakeTuple(%para3_conv1.weight, %para4_bn1.gamma, %para5_bn1.beta, %para6_conv2.weight, %para7_bn2.gamma, %para8_bn2.beta, %para9_conv3.weight, %para10_bn3.gamma, %para11_bn3.beta, %para12_conv4.weight, %para13_bn4.gamma, %para14_bn4.beta, %para15_fc1.weight, %para16_fc1.bias, %para17_fc2.weight, %para18_fc2.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (64, 32, 3, 3)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64, 64, 3, 3)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (128, 64, 3, 3)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 6400)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (3, 128)>, <Ref[Tensor[Float32]], (3)>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_17):S_Prim_grad(%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_17):UnpackCall_unpack_call(%5, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %7(grads) = $(_no_sens_impl_17):call @mindspore_nn_layer_basic_Identity_construct_45(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %8(CNode_46) = $(_no_sens_impl_17):call @mindspore_nn_optim_adam_Adam_construct_47(%7)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %9(loss) = $(_no_sens_impl_17):S_Prim_Depend[side_effect_propagate: I64(1)](%2, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%9)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @_no_sens_impl_19:CNode_51{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core : 1
subgraph instance: UnpackCall_20 : 0000028E6F612360
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
subgraph @UnpackCall_20(%para67_, %para68_) {
  %1(loss) = TupleGetItem(%para68_22, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>, <Int64, NoShape>) -> (<Tensor[Float32], (1, 1, 150, 150)>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(loss) = TupleGetItem(%para68_22, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((1, 1, 150, 150), (1))>, <Int64, NoShape>) -> (<Tensor[Int32], (1)>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/

#------------------------> 6
  %3(loss) = %para67_21(%1, %2)
      : (<Tensor[Float32], (1, 1, 150, 150)>, <Tensor[Int32], (1)>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
}
# Order:
#   1: @UnpackCall_20:loss{[0]: param_21, [1]: loss, [2]: loss}
#   2: @UnpackCall_20:loss{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23 : 0000028E6F35B980
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para69_data, %para70_label) {

#------------------------> 7
  %1(out) = call @mynet_MyNet_construct_24(%para69_data)
      : (<Tensor[Float32], (1, 1, 150, 150)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_53) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_52(%1, %para70_label)
      : (<null>, <Tensor[Int32], (1)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23:out{[0]: ValueNode<FuncGraph> mynet_MyNet_construct_24, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23:CNode_53{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_52, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_23:CNode_54{[0]: ValueNode<Primitive> Return, [1]: CNode_53}


subgraph attr:
training : 1
subgraph instance: mynet_MyNet_construct_24 : 0000028E6F358F00
# In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:59/    def construct(self, x):/
subgraph @mynet_MyNet_construct_24 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para71_x) {
  %1(x) = call @mindspore_nn_layer_conv_Conv2d_construct_55(%para71_x)
      : (<Tensor[Float32], (1, 1, 150, 150)>) -> (<Tensor[Float32], (1, 32, 150, 150)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:63/        x = self.conv1(x)/
  %2(x) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_56(%1)
      : (<Tensor[Float32], (1, 32, 150, 150)>) -> (<Tensor[Float32], (1, 32, 150, 150)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:65/        x = self.bn1(x)/
  %3(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%2)
      : (<Tensor[Float32], (1, 32, 150, 150)>) -> (<Tensor[Float32], (1, 32, 150, 150)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:66/        x = self.relu1(x)/
  %4(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_57(%3)
      : (<Tensor[Float32], (1, 32, 150, 150)>) -> (<Tensor[Float32], (1, 32, 75, 75)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:67/        x = self.maxpool1(x)/
  %5(x) = call @mindspore_nn_layer_conv_Conv2d_construct_58(%4)
      : (<Tensor[Float32], (1, 32, 75, 75)>) -> (<Tensor[Float32], (1, 64, 75, 75)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:70/        x = self.conv2(x)/
  %6(x) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_59(%5)
      : (<Tensor[Float32], (1, 64, 75, 75)>) -> (<Tensor[Float32], (1, 64, 75, 75)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:71/        x = self.bn2(x)/
  %7(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%6)
      : (<Tensor[Float32], (1, 64, 75, 75)>) -> (<Tensor[Float32], (1, 64, 75, 75)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:72/        x = self.relu2(x)/
  %8(x) = call @mindspore_nn_layer_basic_Dropout_construct_60(%7)
      : (<Tensor[Float32], (1, 64, 75, 75)>) -> (<Tensor[Float32], (1, 64, 75, 75)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:73/        x = self.dropout1(x)/
  %9(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_61(%8)
      : (<Tensor[Float32], (1, 64, 75, 75)>) -> (<Tensor[Float32], (1, 64, 38, 38)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:74/        x = self.maxpool2(x)/
  %10(x) = call @mindspore_nn_layer_conv_Conv2d_construct_62(%9)
      : (<Tensor[Float32], (1, 64, 38, 38)>) -> (<Tensor[Float32], (1, 64, 38, 38)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:77/        x = self.conv3(x)/
  %11(x) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_63(%10)
      : (<Tensor[Float32], (1, 64, 38, 38)>) -> (<Tensor[Float32], (1, 64, 38, 38)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:78/        x = self.bn3(x)/
  %12(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%11)
      : (<Tensor[Float32], (1, 64, 38, 38)>) -> (<Tensor[Float32], (1, 64, 38, 38)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:79/        x = self.relu3(x)/
  %13(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_64(%12)
      : (<Tensor[Float32], (1, 64, 38, 38)>) -> (<Tensor[Float32], (1, 64, 19, 19)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:80/        x = self.maxpool3(x)/
  %14(x) = call @mindspore_nn_layer_conv_Conv2d_construct_65(%13)
      : (<Tensor[Float32], (1, 64, 19, 19)>) -> (<Tensor[Float32], (1, 128, 19, 19)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:83/        x = self.conv4(x)/
  %15(x) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_66(%14)
      : (<Tensor[Float32], (1, 128, 19, 19)>) -> (<Tensor[Float32], (1, 128, 19, 19)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:84/        x = self.bn4(x)/
  %16(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%15)
      : (<Tensor[Float32], (1, 128, 19, 19)>) -> (<Tensor[Float32], (1, 128, 19, 19)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:85/        x = self.relu4(x)/
  %17(x) = call @mindspore_nn_layer_basic_Dropout_construct_67(%16)
      : (<Tensor[Float32], (1, 128, 19, 19)>) -> (<Tensor[Float32], (1, 128, 19, 19)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:86/        x = self.dropout2(x)/
  %18(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_68(%17)
      : (<Tensor[Float32], (1, 128, 19, 19)>) -> (<Tensor[Float32], (1, 128, 10, 10)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:87/        x = self.maxpool4(x)/
  %19(x) = call @mindspore_nn_layer_basic_Flatten_construct_69(%18)
      : (<Tensor[Float32], (1, 128, 10, 10)>) -> (<Tensor[Float32], (1, 12800)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:99/        x = self.flatten(x)/

#------------------------> 8
  %20(x) = call @mindspore_nn_layer_basic_Dense_construct_25(%19)
      : (<Tensor[Float32], (1, 12800)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:100/        x = self.fc1(x)/
  %21(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%20)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:102/        x = self.relu6(x)/
  %22(x) = call @mindspore_nn_layer_basic_Dropout_construct_70(%21)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:103/        x = self.dropout4(x)/
  %23(x) = call @mindspore_nn_layer_basic_Dense_construct_71(%22)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:106/        x = self.fc2(x)/
  %24(x) = call @mindspore_nn_layer_activation_Softmax_construct_72(%23)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:108/        x = self.softmax(x)/
  Return(%24)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:111/        return x/
}
# Order:
#   1: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_55, [1]: param_x}
#   2: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_56, [1]: x}
#   3: @mynet_MyNet_construct_24:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#   4: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_57, [1]: x}
#   5: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_58, [1]: x}
#   6: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_59, [1]: x}
#   7: @mynet_MyNet_construct_24:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#   8: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_60, [1]: x}
#   9: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_61, [1]: x}
#  10: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_62, [1]: x}
#  11: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_63, [1]: x}
#  12: @mynet_MyNet_construct_24:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#  13: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_64, [1]: x}
#  14: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_65, [1]: x}
#  15: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_66, [1]: x}
#  16: @mynet_MyNet_construct_24:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#  17: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_67, [1]: x}
#  18: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_68, [1]: x}
#  19: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_69, [1]: x}
#  20: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_25, [1]: x}
#  21: @mynet_MyNet_construct_24:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#  22: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_70, [1]: x}
#  23: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_71, [1]: x}
#  24: @mynet_MyNet_construct_24:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_Softmax_construct_72, [1]: x}
#  25: @mynet_MyNet_construct_24:CNode_73{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_25 : 0000028E6F5F6520
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_25 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para72_x) {

#------------------------> 9
  %1(CNode_74) = call @L_mindspore_nn_layer_basic_Dense_construct_26(%para72_x, %para16_fc1.bias, %para15_fc1.weight)
      : (<Tensor[Float32], (1, 12800)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 6400)>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc1-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_25:CNode_74{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_26, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_25:CNode_75{[0]: ValueNode<Primitive> Return, [1]: CNode_74}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_26 : 0000028E6F5F9F90
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_26(%para73_x, %para74_, %para75_) {
  %1(x_shape) = S_Prim_Shape(%para73_x)
      : (<Tensor[Float32], (1, 12800)>) -> (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_76) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_77) = StopGradient(%2)
      : (<None, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
  %4(CNode_78) = S_Prim_inner_len(%1)
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_79) = S_Prim_not_equal(%4, I64(2))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_80) = Cond(%5, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_81) = Switch(%6, @L_mindspore_nn_layer_basic_Dense_construct_82, @L_mindspore_nn_layer_basic_Dense_construct_83)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_84) = %7()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/

#------------------------> 10
  %9(CNode_85) = call @L_mindspore_nn_layer_basic_Dense_construct_27(%8)
      : (<Tensor[Float32], (1, 12800)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:106/        x = self.fc2(x)/
  %10(CNode_86) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <None, NoShape>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:106/        x = self.fc2(x)/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_26:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_76{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_78{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_79{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_78, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_80{[0]: ValueNode<Primitive> Cond, [1]: CNode_79, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_81{[0]: ValueNode<Primitive> Switch, [1]: CNode_80, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_82, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_83}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_84{[0]: CNode_81}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_85{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_27, [1]: CNode_84}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_86{[0]: ValueNode<Primitive> Depend, [1]: CNode_85, [2]: CNode_77}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_26:CNode_87{[0]: ValueNode<Primitive> Return, [1]: CNode_86}


subgraph attr:
training : 1
after_block : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_27 : 0000028E6F5F6A70
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_27 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_26](%para76_) {

#------------------------> 11
  %1(CNode_88) = call @L_mindspore_nn_layer_basic_Dense_construct_28()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_27:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_phi_x, [2]: param_L_fc2.weight}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_27:CNode_88{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_28}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_27:CNode_89{[0]: ValueNode<Primitive> Return, [1]: CNode_88}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_28 : 0000028E6F5F6FC0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_28 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_27]() {

#------------------------> 12
  %1(CNode_90) = call @L_mindspore_nn_layer_basic_Dense_construct_29()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_28:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc2.bias}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_28:CNode_90{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_29}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_28:CNode_91{[0]: ValueNode<Primitive> Return, [1]: CNode_90}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_29 : 0000028E6F5F7A60
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_29 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_28]() {

#------------------------> 13
  %1(CNode_92) = call @L_mindspore_nn_layer_basic_Dense_construct_30()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_29:CNode_92{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_30}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_29:CNode_93{[0]: ValueNode<Primitive> Return, [1]: CNode_92}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_30 : 0000028E6F5FBF70
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_30 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_28]() {

#------------------------> 14
  %1(CNode_94) = call @L_mindspore_nn_layer_basic_Dense_construct_31()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_30:CNode_94{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_31}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_30:CNode_95{[0]: ValueNode<Primitive> Return, [1]: CNode_94}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_31 : 0000028E6F5FCA10
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_31 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_28]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_26):S_Prim_Shape(%para73_x)
      : (<Tensor[Float32], (1, 12800)>) -> (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_96) = S_Prim_inner_len(%1)
      : (<Tuple[Int64*2], TupleShape(NoShape, NoShape)>) -> (<Int64, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_97) = S_Prim_not_equal(%2, I64(2))
      : (<Int64, NoShape>, <Int64, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_98) = Cond(%3, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_99) = Switch(%4, @L_mindspore_nn_layer_basic_Dense_construct_100, @L_mindspore_nn_layer_basic_Dense_construct_32)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/

#------------------------> 15
  %6(CNode_101) = %5()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_103) = call @L_mindspore_nn_layer_basic_Dense_construct_102(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:106/        x = self.fc2(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_31:CNode_96{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_31:CNode_97{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_96, [2]: ValueNode<Int64Imm> 2}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_31:CNode_98{[0]: ValueNode<Primitive> Cond, [1]: CNode_97, [2]: ValueNode<BoolImm> false}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_31:CNode_99{[0]: ValueNode<Primitive> Switch, [1]: CNode_98, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_100, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_32}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_31:CNode_101{[0]: CNode_99}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_31:CNode_103{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_102, [1]: CNode_101}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_31:CNode_104{[0]: ValueNode<Primitive> Return, [1]: CNode_103}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_32 : 0000028E6F5F4FE0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_32 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_28]() {

#------------------------> 16
  %1(x) = $(L_mindspore_nn_layer_basic_Dense_construct_27):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para76_phi_x, %para75_L_fc2.weight)
      : (<Tensor[Float32], (1, 12800)>, <Ref[Tensor[Float32]], (128, 6400)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_mindspore_nn_layer_basic_Dense_construct_28):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"]](%1, %para74_L_fc2.bias)
      : (<null>, <Ref[Tensor[Float32]], (128)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_32:CNode_105{[0]: ValueNode<Primitive> Return, [1]: x}


# ===============================================================================================
# The total of function graphs in evaluation stack: 17/18 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
subgraph attr:
training : 1
subgraph instance: _no_sens_impl_37 : 0000028E6F117B30
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_37 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para77_inputs) {
  %1(CNode_40) = call @_no_sens_impl_106()
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_37:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.41, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_107, [2]: param_inputs}
#   2: @_no_sens_impl_37:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_107, [2]: param_inputs}
#   3: @_no_sens_impl_37:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_43}
#   4: @_no_sens_impl_37:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.44, [1]: grads, [2]: param_inputs}
#   5: @_no_sens_impl_37:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_108, [1]: grads}
#   6: @_no_sens_impl_37:CNode_46{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_109, [1]: grads}
#   7: @_no_sens_impl_37:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_46}
#   8: @_no_sens_impl_37:CNode_40{[0]: ValueNode<FuncGraph> _no_sens_impl_106}
#   9: @_no_sens_impl_37:CNode_48{[0]: ValueNode<Primitive> Return, [1]: CNode_40}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_109 : 0000028E6F1175E0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_109 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para78_gradients) {
  %1(CNode_111) = call @mindspore_nn_optim_adam_Adam_construct_110()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:916/        if not self.use_offload:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:916/        if not self.use_offload:/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_109:gradients{[0]: ValueNode<FuncGraph> flatten_gradients_112, [1]: param_gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_109:gradients{[0]: ValueNode<FuncGraph> decay_weight_113, [1]: gradients}
#   3: @mindspore_nn_optim_adam_Adam_construct_109:CNode_111{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_110}
#   4: @mindspore_nn_optim_adam_Adam_construct_109:CNode_114{[0]: ValueNode<Primitive> Return, [1]: CNode_111}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Identity_construct_108 : 0000028E6F110650
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:505/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Identity_construct_108(%para79_x) {
  Return(%para79_x)
      : (<null>)
      #scope: (Default/grad_reducer-Identity)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:506/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Identity_construct_108:CNode_115{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_107 : 0000028E6F11C590
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_107 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para80_data, %para81_label) {
  %1(out) = call @mynet_MyNet_construct_116(%para80_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_53) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117(%1, %para81_label)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_107:out{[0]: ValueNode<FuncGraph> mynet_MyNet_construct_116, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_107:CNode_53{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_107:CNode_54{[0]: ValueNode<Primitive> Return, [1]: CNode_53}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_106 : 0000028E6F3514D0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_106 parent: [subgraph @_no_sens_impl_37]() {
  %1(CNode_49) = call @_no_sens_impl_118()
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_106:CNode_49{[0]: ValueNode<FuncGraph> _no_sens_impl_118}
#   2: @_no_sens_impl_106:CNode_50{[0]: ValueNode<Primitive> Return, [1]: CNode_49}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_113 : 0000028E6F115B50
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_113(%para82_gradients) {
  %1(CNode_120) = call @decay_weight_119()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_113:CNode_120{[0]: ValueNode<FuncGraph> decay_weight_119}
#   2: @decay_weight_113:CNode_121{[0]: ValueNode<Primitive> Return, [1]: CNode_120}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_112 : 0000028E6F113B70
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_112(%para83_gradients) {
  %1(CNode_123) = call @flatten_gradients_122()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_112:CNode_123{[0]: ValueNode<FuncGraph> flatten_gradients_122}
#   2: @flatten_gradients_112:CNode_124{[0]: ValueNode<Primitive> Return, [1]: CNode_123}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_110 : 0000028E6F112630
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_110 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_109]() {
  %1(CNode_126) = call @mindspore_nn_optim_adam_Adam_construct_125()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:917/            gradients = self.gradients_centralization(gradients)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:917/            gradients = self.gradients_centralization(gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_110:gradients{[0]: ValueNode<FuncGraph> gradients_centralization_127, [1]: gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_110:CNode_126{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_125}
#   3: @mindspore_nn_optim_adam_Adam_construct_110:CNode_128{[0]: ValueNode<Primitive> Return, [1]: CNode_126}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117 : 0000028E6F34CFC0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117(%para84_logits, %para85_labels) {
  %1(CNode_129) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para84_logits, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_130) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para85_labels, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_131) = MakeTuple(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_132) = StopGradient(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %5(CNode_134) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  %6(CNode_135) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117:CNode_129{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117:CNode_130{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117:CNode_134{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117:CNode_136{[0]: ValueNode<Primitive> Return, [1]: CNode_135}


subgraph attr:
training : 1
subgraph instance: mynet_MyNet_construct_116 : 0000028E6F11BAF0
# In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:59/    def construct(self, x):/
subgraph @mynet_MyNet_construct_116 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para86_x) {
  %1(x) = call @mindspore_nn_layer_conv_Conv2d_construct_137(%para86_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:63/        x = self.conv1(x)/
  %2(x) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_138(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:65/        x = self.bn1(x)/
  %3(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:66/        x = self.relu1(x)/
  %4(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_139(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:67/        x = self.maxpool1(x)/
  %5(x) = call @mindspore_nn_layer_conv_Conv2d_construct_140(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:70/        x = self.conv2(x)/
  %6(x) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_141(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:71/        x = self.bn2(x)/
  %7(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:72/        x = self.relu2(x)/
  %8(x) = call @mindspore_nn_layer_basic_Dropout_construct_142(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:73/        x = self.dropout1(x)/
  %9(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_143(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:74/        x = self.maxpool2(x)/
  %10(x) = call @mindspore_nn_layer_conv_Conv2d_construct_144(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:77/        x = self.conv3(x)/
  %11(x) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_145(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:78/        x = self.bn3(x)/
  %12(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:79/        x = self.relu3(x)/
  %13(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_146(%12)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:80/        x = self.maxpool3(x)/
  %14(x) = call @mindspore_nn_layer_conv_Conv2d_construct_147(%13)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:83/        x = self.conv4(x)/
  %15(x) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_148(%14)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:84/        x = self.bn4(x)/
  %16(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%15)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:85/        x = self.relu4(x)/
  %17(x) = call @mindspore_nn_layer_basic_Dropout_construct_149(%16)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:86/        x = self.dropout2(x)/
  %18(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_150(%17)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:87/        x = self.maxpool4(x)/
  %19(x) = call @mindspore_nn_layer_basic_Flatten_construct_151(%18)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:99/        x = self.flatten(x)/
  %20(x) = call @mindspore_nn_layer_basic_Dense_construct_152(%19)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:100/        x = self.fc1(x)/
  %21(x) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%20)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:102/        x = self.relu6(x)/
  %22(x) = call @mindspore_nn_layer_basic_Dropout_construct_153(%21)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:103/        x = self.dropout4(x)/
  %23(x) = call @mindspore_nn_layer_basic_Dense_construct_154(%22)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:106/        x = self.fc2(x)/
  %24(x) = call @mindspore_nn_layer_activation_Softmax_construct_155(%23)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:108/        x = self.softmax(x)/
  Return(%24)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:111/        return x/
}
# Order:
#   1: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_137, [1]: param_x}
#   2: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_138, [1]: x}
#   3: @mynet_MyNet_construct_116:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#   4: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_139, [1]: x}
#   5: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_140, [1]: x}
#   6: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_141, [1]: x}
#   7: @mynet_MyNet_construct_116:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#   8: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_142, [1]: x}
#   9: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_143, [1]: x}
#  10: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_144, [1]: x}
#  11: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_145, [1]: x}
#  12: @mynet_MyNet_construct_116:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#  13: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_146, [1]: x}
#  14: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_147, [1]: x}
#  15: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_148, [1]: x}
#  16: @mynet_MyNet_construct_116:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#  17: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_149, [1]: x}
#  18: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_150, [1]: x}
#  19: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_151, [1]: x}
#  20: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_152, [1]: x}
#  21: @mynet_MyNet_construct_116:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: x}
#  22: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_153, [1]: x}
#  23: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_154, [1]: x}
#  24: @mynet_MyNet_construct_116:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_Softmax_construct_155, [1]: x}
#  25: @mynet_MyNet_construct_116:CNode_73{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_118 : 0000028E6F352A10
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_118 parent: [subgraph @_no_sens_impl_37]() {
  %1(loss) = $(_no_sens_impl_37):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_107, %para77_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(grads) = $(_no_sens_impl_37):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_107, %para77_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %3(CNode_43) = $(_no_sens_impl_37):MakeTuple(%para3_conv1.weight, %para4_bn1.gamma, %para5_bn1.beta, %para6_conv2.weight, %para7_bn2.gamma, %para8_bn2.beta, %para9_conv3.weight, %para10_bn3.gamma, %para11_bn3.beta, %para12_conv4.weight, %para13_bn4.gamma, %para14_bn4.beta, %para15_fc1.weight, %para16_fc1.bias, %para17_fc2.weight, %para18_fc2.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn2.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:conv3.weight>, <Ref[Tensor[Float32]], (64), ref_key=:bn3.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn3.beta>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:conv4.weight>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (128, 6400), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (3, 128), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (3), ref_key=:fc2.bias>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(grads) = $(_no_sens_impl_37):S_Prim_grad(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_37):UnpackCall_unpack_call(%4, %para77_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_37):call @mindspore_nn_layer_basic_Identity_construct_108(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %7(CNode_46) = $(_no_sens_impl_37):call @mindspore_nn_optim_adam_Adam_construct_109(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %8(loss) = $(_no_sens_impl_37):S_Prim_Depend[side_effect_propagate: I64(1)](%1, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @_no_sens_impl_118:CNode_51{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_119 : 0000028E6F111B90
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_119 parent: [subgraph @decay_weight_113]() {
  %1(CNode_157) = call @decay_weight_156()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_119:CNode_157{[0]: ValueNode<FuncGraph> decay_weight_156}
#   2: @decay_weight_119:CNode_158{[0]: ValueNode<Primitive> Return, [1]: CNode_157}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_122 : 0000028E6F1130D0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_122 parent: [subgraph @flatten_gradients_112]() {
  %1(CNode_160) = call @flatten_gradients_159()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_122:CNode_160{[0]: ValueNode<FuncGraph> flatten_gradients_159}
#   2: @flatten_gradients_122:CNode_161{[0]: ValueNode<Primitive> Return, [1]: CNode_160}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_127 : 0000028E6F1160A0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_127(%para87_gradients) {
  %1(CNode_163) = call @gradients_centralization_162()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_127:CNode_163{[0]: ValueNode<FuncGraph> gradients_centralization_162}
#   2: @gradients_centralization_127:CNode_164{[0]: ValueNode<Primitive> Return, [1]: CNode_163}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_125 : 0000028E6F1110F0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_125 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_110]() {
  %1(CNode_165) = S_Prim_AssignAdd[input_names: ["ref", "value"], output_names: ["ref"], side_effect_mem: Bool(1)](%para53_global_step, Tensor(shape=[1], dtype=Int32, value=[1]))
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:921/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(beta1_power) = S_Prim_mul(%para51_beta1_power, Tensor(shape=[], dtype=Float32, value=0.9))
      : (<Ref[Tensor[Float32]], (), ref_key=:beta1_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:923/        beta1_power = self.beta1_power * self.beta1/
  %3(CNode_167) = call @assign_166(%para51_beta1_power, %2)
      : (<Ref[Tensor[Float32]], (), ref_key=:beta1_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:924/        self.beta1_power = beta1_power/
  %4(beta2_power) = S_Prim_mul(%para52_beta2_power, Tensor(shape=[], dtype=Float32, value=0.999))
      : (<Ref[Tensor[Float32]], (), ref_key=:beta2_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:925/        beta2_power = self.beta2_power * self.beta2/
  %5(CNode_168) = call @assign_166(%para52_beta2_power, %4)
      : (<Ref[Tensor[Float32]], (), ref_key=:beta2_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:926/        self.beta2_power = beta2_power/
  %6(CNode_169) = MakeTuple(%1, %3, %5)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:909/    @jit/
  %7(CNode_170) = StopGradient(%6)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:909/    @jit/
  %8(CNode_171) = $(mindspore_nn_optim_adam_Adam_construct_109):MakeTuple(%para3_conv1.weight, %para4_bn1.gamma, %para5_bn1.beta, %para6_conv2.weight, %para7_bn2.gamma, %para8_bn2.beta, %para9_conv3.weight, %para10_bn3.gamma, %para11_bn3.beta, %para12_conv4.weight, %para13_bn4.gamma, %para14_bn4.beta, %para15_fc1.weight, %para16_fc1.bias, %para17_fc2.weight, %para18_fc2.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn2.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:conv3.weight>, <Ref[Tensor[Float32]], (64), ref_key=:bn3.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn3.beta>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:conv4.weight>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (128, 6400), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (3, 128), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (3), ref_key=:fc2.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:911/        params = self._parameters/
  %9(CNode_172) = $(mindspore_nn_optim_adam_Adam_construct_109):MakeTuple(%para19_moment1.conv1.weight, %para20_moment1.bn1.gamma, %para21_moment1.bn1.beta, %para22_moment1.conv2.weight, %para23_moment1.bn2.gamma, %para24_moment1.bn2.beta, %para25_moment1.conv3.weight, %para26_moment1.bn3.gamma, %para27_moment1.bn3.beta, %para28_moment1.conv4.weight, %para29_moment1.bn4.gamma, %para30_moment1.bn4.beta, %para31_moment1.fc1.weight, %para32_moment1.fc1.bias, %para33_moment1.fc2.weight, %para34_moment1.fc2.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment1.conv1.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.bn1.beta>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:moment1.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn2.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moment1.conv3.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn3.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.bn3.beta>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:moment1.conv4.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.bn4.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.bn4.beta>, <Ref[Tensor[Float32]], (128, 6400), ref_key=:moment1.fc1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.fc1.bias>, <Ref[Tensor[Float32]], (3, 128), ref_key=:moment1.fc2.weight>, <Ref[Tensor[Float32]], (3), ref_key=:moment1.fc2.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:912/        moment1 = self.moment1/
  %10(CNode_173) = $(mindspore_nn_optim_adam_Adam_construct_109):MakeTuple(%para35_moment2.conv1.weight, %para36_moment2.bn1.gamma, %para37_moment2.bn1.beta, %para38_moment2.conv2.weight, %para39_moment2.bn2.gamma, %para40_moment2.bn2.beta, %para41_moment2.conv3.weight, %para42_moment2.bn3.gamma, %para43_moment2.bn3.beta, %para44_moment2.conv4.weight, %para45_moment2.bn4.gamma, %para46_moment2.bn4.beta, %para47_moment2.fc1.weight, %para48_moment2.fc1.bias, %para49_moment2.fc2.weight, %para50_moment2.fc2.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment2.conv1.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.bn1.beta>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:moment2.conv2.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn2.beta>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:moment2.conv3.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn3.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.bn3.beta>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:moment2.conv4.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.bn4.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.bn4.beta>, <Ref[Tensor[Float32]], (128, 6400), ref_key=:moment2.fc1.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.fc1.bias>, <Ref[Tensor[Float32]], (3, 128), ref_key=:moment2.fc2.weight>, <Ref[Tensor[Float32]], (3), ref_key=:moment2.fc2.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:913/        moment2 = self.moment2/
  %11(lr) = call @get_lr_174()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:920/        lr = self.get_lr()/
  %12(gradients) = $(mindspore_nn_optim_adam_Adam_construct_109):call @flatten_gradients_112(%para78_gradients)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:914/        gradients = self.flatten_gradients(gradients)/
  %13(gradients) = $(mindspore_nn_optim_adam_Adam_construct_109):call @decay_weight_113(%12)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:915/        gradients = self.decay_weight(gradients)/
  %14(gradients) = $(mindspore_nn_optim_adam_Adam_construct_110):call @gradients_centralization_127(%13)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:917/            gradients = self.gradients_centralization(gradients)/
  %15(gradients) = call @scale_grad_175(%14)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:918/        gradients = self.scale_grad(gradients)/
  %16(gradients) = call @_grad_sparse_indices_deduplicate_176(%15)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:919/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  %17(CNode_178) = call @_apply_adam_177(%8, %2, %4, %9, %10, %11, %16)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  %18(CNode_179) = Depend[side_effect_propagate: I64(1)](%17, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  Return(%18)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_125:gradients{[0]: ValueNode<FuncGraph> scale_grad_175, [1]: gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_125:gradients{[0]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_176, [1]: gradients}
#   3: @mindspore_nn_optim_adam_Adam_construct_125:lr{[0]: ValueNode<FuncGraph> get_lr_174}
#   4: @mindspore_nn_optim_adam_Adam_construct_125:CNode_165{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AssignAdd, [1]: param_global_step, [2]: ValueNode<Tensor> Tensor(shape=[1], dtype=Int32, value=[1])}
#   5: @mindspore_nn_optim_adam_Adam_construct_125:beta1_power{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_beta1_power, [2]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.9)}
#   6: @mindspore_nn_optim_adam_Adam_construct_125:CNode_167{[0]: ValueNode<FuncGraph> assign_166, [1]: param_beta1_power, [2]: beta1_power}
#   7: @mindspore_nn_optim_adam_Adam_construct_125:beta2_power{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_beta2_power, [2]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.999)}
#   8: @mindspore_nn_optim_adam_Adam_construct_125:CNode_168{[0]: ValueNode<FuncGraph> assign_166, [1]: param_beta2_power, [2]: beta2_power}
#   9: @mindspore_nn_optim_adam_Adam_construct_125:CNode_178{[0]: ValueNode<FuncGraph> _apply_adam_177, [1]: CNode_171, [2]: beta1_power, [3]: beta2_power, [4]: CNode_172, [5]: CNode_173, [6]: lr, [7]: gradients}
#  10: @mindspore_nn_optim_adam_Adam_construct_125:CNode_180{[0]: ValueNode<Primitive> Return, [1]: CNode_179}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133 : 0000028E6F34D510
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117]() {
  %1(CNode_181) = S_Prim_equal("mean", "mean")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_182) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_183) = Switch(%2, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_184, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_185)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %4(CNode_186) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133:CNode_181{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133:CNode_182{[0]: ValueNode<Primitive> Cond, [1]: CNode_181, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133:CNode_183{[0]: ValueNode<Primitive> Switch, [1]: CNode_182, [2]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_184, [3]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_185}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133:CNode_186{[0]: CNode_183}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_133:CNode_187{[0]: ValueNode<Primitive> Return, [1]: CNode_186}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_Softmax_construct_155 : 0000028E6F34C520
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py:277/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_Softmax_construct_155(%para88_x) {
  %1(CNode_188) = S_Prim_Softmax[axis: (I64(1)), input_names: ["x"], output_names: ["output"]](%para88_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/softmax-Softmax)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py:278/        return self.softmax(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/softmax-Softmax)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\activation.py:278/        return self.softmax(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_Softmax_construct_155:CNode_188{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Softmax, [1]: param_x}
#   2: @mindspore_nn_layer_activation_Softmax_construct_155:CNode_189{[0]: ValueNode<Primitive> Return, [1]: CNode_188}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_154 : 0000028E6F34BA80
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_154 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para89_x) {
  %1(CNode_191) = call @L_mindspore_nn_layer_basic_Dense_construct_190(%para89_x, %para18_fc2.bias, %para17_fc2.weight)
      : (<null>, <Ref[Tensor[Float32]], (3), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (3, 128), ref_key=:fc2.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_154:CNode_191{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_190, [1]: param_x, [2]: param_fc2.bias, [3]: param_fc2.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_154:CNode_87{[0]: ValueNode<Primitive> Return, [1]: CNode_191}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_153 : 0000028E6F34B530
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_153(%para90_x) {
  %1(CNode_192) = S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_193) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_194) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_195, @mindspore_nn_layer_basic_Dropout_construct_196)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_197) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %5(CNode_198) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %6(CNode_199) = Switch(%5, @mindspore_nn_layer_basic_Dropout_construct_200, @mindspore_nn_layer_basic_Dropout_construct_201)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %7(CNode_202) = %6()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_153:CNode_192{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: ValueNode<BoolImm> true}
#   2: @mindspore_nn_layer_basic_Dropout_construct_153:CNode_193{[0]: ValueNode<Primitive> Cond, [1]: CNode_192, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_153:CNode_194{[0]: ValueNode<Primitive> Switch, [1]: CNode_193, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_195, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_196}
#   4: @mindspore_nn_layer_basic_Dropout_construct_153:CNode_197{[0]: CNode_194}
#   5: @mindspore_nn_layer_basic_Dropout_construct_153:CNode_198{[0]: ValueNode<Primitive> Cond, [1]: CNode_197, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dropout_construct_153:CNode_199{[0]: ValueNode<Primitive> Switch, [1]: CNode_198, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_200, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_201}
#   7: @mindspore_nn_layer_basic_Dropout_construct_153:CNode_202{[0]: CNode_199}
#   8: @mindspore_nn_layer_basic_Dropout_construct_153:CNode_203{[0]: ValueNode<Primitive> Return, [1]: CNode_202}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_152 : 0000028E6F3445A0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_152 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para91_x) {
  %1(CNode_74) = call @L_mindspore_nn_layer_basic_Dense_construct_190(%para91_x, %para16_fc1.bias, %para15_fc1.weight)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (128, 6400), ref_key=:fc1.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc1-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_152:CNode_74{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_190, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_152:CNode_75{[0]: ValueNode<Primitive> Return, [1]: CNode_74}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_151 : 0000028E6F1F7520
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Flatten_construct_151(%para92_x) {
  %1(x_rank) = call @rank_204(%para92_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:462/        x_rank = F.rank(x)/
  %2(CNode_205) = S_Prim_not_equal(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_206) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %4(CNode_207) = Switch(%3, @mindspore_nn_layer_basic_Flatten_construct_208, @mindspore_nn_layer_basic_Flatten_construct_209)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %5(ndim) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %6(CNode_211) = call @check_axis_valid_210(I64(1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:464/        self.check_axis_valid(self.start_dim, ndim)/
  %7(CNode_212) = call @check_axis_valid_210(I64(-1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:465/        self.check_axis_valid(self.end_dim, ndim)/
  %8(CNode_213) = MakeTuple(%6, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
  %9(CNode_214) = StopGradient(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
  %10(CNode_215) = S_Prim_MakeTuple(%para92_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %11(CNode_216) = S_Prim_MakeTuple("start_dim", "end_dim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %12(CNode_217) = S_Prim_MakeTuple(I64(1), I64(-1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %13(CNode_218) = S_Prim_make_dict(%11, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %14(CNode_219) = UnpackCall_unpack_call(@flatten_220, %10, %13)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %15(CNode_221) = Depend[side_effect_propagate: I64(1)](%14, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_151:x_rank{[0]: ValueNode<FuncGraph> rank_204, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_205{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: x_rank, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_206{[0]: ValueNode<Primitive> Cond, [1]: CNode_205, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_207{[0]: ValueNode<Primitive> Switch, [1]: CNode_206, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_208, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_209}
#   5: @mindspore_nn_layer_basic_Flatten_construct_151:ndim{[0]: CNode_207}
#   6: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_211{[0]: ValueNode<FuncGraph> check_axis_valid_210, [1]: ValueNode<Int64Imm> 1, [2]: ndim}
#   7: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_212{[0]: ValueNode<FuncGraph> check_axis_valid_210, [1]: ValueNode<Int64Imm> -1, [2]: ndim}
#   8: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_215{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_x}
#   9: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_216{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> start_dim, [2]: ValueNode<StringImm> end_dim}
#  10: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_217{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1, [2]: ValueNode<Int64Imm> -1}
#  11: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_218{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_216, [2]: CNode_217}
#  12: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_219{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.222, [1]: ValueNode<FuncGraph> flatten_220, [2]: CNode_215, [3]: CNode_218}
#  13: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_223{[0]: ValueNode<Primitive> Return, [1]: CNode_221}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_150 : 0000028E6F1FD4C0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_150(%para93_x) {
  %1(CNode_224) = getattr(%para93_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_225) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_226) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_227) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_228, @mindspore_nn_layer_pooling_MaxPool2d_construct_229)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_230) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_150:CNode_224{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_150:CNode_225{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_224, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_150:CNode_226{[0]: ValueNode<Primitive> Cond, [1]: CNode_225, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_150:CNode_227{[0]: ValueNode<Primitive> Switch, [1]: CNode_226, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_228, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_229}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_150:CNode_230{[0]: CNode_227}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_150:CNode_231{[0]: ValueNode<Primitive> Return, [1]: CNode_230}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_149 : 0000028E6F1FB4E0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_149(%para94_x) {
  %1(CNode_232) = S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_233) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_234) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_235, @mindspore_nn_layer_basic_Dropout_construct_236)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_237) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %5(CNode_238) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %6(CNode_239) = Switch(%5, @mindspore_nn_layer_basic_Dropout_construct_240, @mindspore_nn_layer_basic_Dropout_construct_241)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %7(CNode_242) = %6()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_149:CNode_232{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: ValueNode<BoolImm> true}
#   2: @mindspore_nn_layer_basic_Dropout_construct_149:CNode_233{[0]: ValueNode<Primitive> Cond, [1]: CNode_232, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_149:CNode_234{[0]: ValueNode<Primitive> Switch, [1]: CNode_233, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_235, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_236}
#   4: @mindspore_nn_layer_basic_Dropout_construct_149:CNode_237{[0]: CNode_234}
#   5: @mindspore_nn_layer_basic_Dropout_construct_149:CNode_238{[0]: ValueNode<Primitive> Cond, [1]: CNode_237, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dropout_construct_149:CNode_239{[0]: ValueNode<Primitive> Switch, [1]: CNode_238, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_240, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_241}
#   7: @mindspore_nn_layer_basic_Dropout_construct_149:CNode_242{[0]: CNode_239}
#   8: @mindspore_nn_layer_basic_Dropout_construct_149:CNode_243{[0]: ValueNode<Primitive> Return, [1]: CNode_242}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_148 : 0000028E6F1F5540
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_148 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para95_x) {
  %1(CNode_244) = S_Prim_Shape(%para95_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_245) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_246) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_247) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_248) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_249) = Switch(%5, @mindspore_nn_layer_normalization_BatchNorm2d_construct_250, @mindspore_nn_layer_normalization_BatchNorm2d_construct_251)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_252) = %6()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_253) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_148:CNode_244{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_148:CNode_245{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_244, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_148:CNode_247{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_148:CNode_248{[0]: ValueNode<Primitive> Cond, [1]: CNode_247, [2]: ValueNode<BoolImm> false}
#   5: @mindspore_nn_layer_normalization_BatchNorm2d_construct_148:CNode_249{[0]: ValueNode<Primitive> Switch, [1]: CNode_248, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_250, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_251}
#   6: @mindspore_nn_layer_normalization_BatchNorm2d_construct_148:CNode_252{[0]: CNode_249}
#   7: @mindspore_nn_layer_normalization_BatchNorm2d_construct_148:CNode_254{[0]: ValueNode<Primitive> Return, [1]: CNode_253}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_147 : 0000028E6F1F3010
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_147 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para96_x) {
  %1(CNode_256) = call @mindspore_nn_layer_conv_Conv2d_construct_255()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv4-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv4-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_147:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv4.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_147:CNode_256{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_255}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_147:CNode_257{[0]: ValueNode<Primitive> Return, [1]: CNode_256}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_146 : 0000028E6F1F2AC0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_146(%para97_x) {
  %1(CNode_258) = getattr(%para97_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_259) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_260) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_261) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_262, @mindspore_nn_layer_pooling_MaxPool2d_construct_263)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_264) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_146:CNode_258{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_146:CNode_259{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_258, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_146:CNode_260{[0]: ValueNode<Primitive> Cond, [1]: CNode_259, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_146:CNode_261{[0]: ValueNode<Primitive> Switch, [1]: CNode_260, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_262, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_263}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_146:CNode_264{[0]: CNode_261}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_146:CNode_265{[0]: ValueNode<Primitive> Return, [1]: CNode_264}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_145 : 0000028E6F1EF5A0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_145 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para98_x) {
  %1(CNode_267) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266(%para98_x, %para10_bn3.gamma, %para11_bn3.beta, %para57_bn3.moving_mean, %para58_bn3.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:bn3.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn3.beta>, <Ref[Tensor[Float32]], (64), ref_key=:bn3.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:bn3.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_145:CNode_267{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266, [1]: param_x, [2]: param_bn3.gamma, [3]: param_bn3.beta, [4]: param_bn3.moving_mean, [5]: param_bn3.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_145:CNode_268{[0]: ValueNode<Primitive> Return, [1]: CNode_267}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_144 : 0000028E6F1F1030
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_144 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para99_x) {
  %1(CNode_270) = call @mindspore_nn_layer_conv_Conv2d_construct_269()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv3-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv3-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_144:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv3.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_144:CNode_270{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_269}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_144:CNode_271{[0]: ValueNode<Primitive> Return, [1]: CNode_270}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_143 : 0000028E6F12D9D0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_143(%para100_x) {
  %1(CNode_272) = getattr(%para100_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_273) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_274) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_275) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_276, @mindspore_nn_layer_pooling_MaxPool2d_construct_277)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_278) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_143:CNode_272{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_143:CNode_273{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_272, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_143:CNode_274{[0]: ValueNode<Primitive> Cond, [1]: CNode_273, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_143:CNode_275{[0]: ValueNode<Primitive> Switch, [1]: CNode_274, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_276, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_277}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_143:CNode_278{[0]: CNode_275}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_143:CNode_279{[0]: ValueNode<Primitive> Return, [1]: CNode_278}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_142 : 0000028E6F125FA0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_142(%para101_x) {
  %1(CNode_280) = S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_281) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_282) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_283, @mindspore_nn_layer_basic_Dropout_construct_284)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_285) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %5(CNode_286) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %6(CNode_287) = Switch(%5, @mindspore_nn_layer_basic_Dropout_construct_288, @mindspore_nn_layer_basic_Dropout_construct_289)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %7(CNode_290) = %6()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_142:CNode_280{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: ValueNode<BoolImm> true}
#   2: @mindspore_nn_layer_basic_Dropout_construct_142:CNode_281{[0]: ValueNode<Primitive> Cond, [1]: CNode_280, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_142:CNode_282{[0]: ValueNode<Primitive> Switch, [1]: CNode_281, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_283, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_284}
#   4: @mindspore_nn_layer_basic_Dropout_construct_142:CNode_285{[0]: CNode_282}
#   5: @mindspore_nn_layer_basic_Dropout_construct_142:CNode_286{[0]: ValueNode<Primitive> Cond, [1]: CNode_285, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dropout_construct_142:CNode_287{[0]: ValueNode<Primitive> Switch, [1]: CNode_286, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_288, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_289}
#   7: @mindspore_nn_layer_basic_Dropout_construct_142:CNode_290{[0]: CNode_287}
#   8: @mindspore_nn_layer_basic_Dropout_construct_142:CNode_291{[0]: ValueNode<Primitive> Return, [1]: CNode_290}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_141 : 0000028E6F12CF30
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_141 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para102_x) {
  %1(CNode_292) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266(%para102_x, %para7_bn2.gamma, %para8_bn2.beta, %para59_bn2.moving_mean, %para60_bn2.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:bn2.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:bn2.beta>, <Ref[Tensor[Float32]], (64), ref_key=:bn2.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:bn2.moving_variance>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn2-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_141:CNode_292{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266, [1]: param_x, [2]: param_bn2.gamma, [3]: param_bn2.beta, [4]: param_bn2.moving_mean, [5]: param_bn2.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_141:CNode_293{[0]: ValueNode<Primitive> Return, [1]: CNode_292}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_140 : 0000028E6F1294C0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_140 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para103_x) {
  %1(CNode_295) = call @mindspore_nn_layer_conv_Conv2d_construct_294()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv2-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv2-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_140:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_140:CNode_295{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_294}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_140:CNode_296{[0]: ValueNode<Primitive> Return, [1]: CNode_295}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_139 : 0000028E6F11E020
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_139(%para104_x) {
  %1(CNode_297) = getattr(%para104_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_298) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_299) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_300) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_301, @mindspore_nn_layer_pooling_MaxPool2d_construct_302)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_303) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_139:CNode_297{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_139:CNode_298{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_297, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_139:CNode_299{[0]: ValueNode<Primitive> Cond, [1]: CNode_298, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_139:CNode_300{[0]: ValueNode<Primitive> Switch, [1]: CNode_299, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_301, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_302}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_139:CNode_303{[0]: CNode_300}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_139:CNode_304{[0]: ValueNode<Primitive> Return, [1]: CNode_303}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_138 : 0000028E6F11FAB0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_138 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para105_x) {
  %1(CNode_305) = S_Prim_Shape(%para105_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_306) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_307) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_308) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_309) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_310) = Switch(%5, @mindspore_nn_layer_normalization_BatchNorm2d_construct_311, @mindspore_nn_layer_normalization_BatchNorm2d_construct_312)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_313) = %6()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_314) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_138:CNode_305{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_138:CNode_306{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_305, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_138:CNode_308{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_138:CNode_309{[0]: ValueNode<Primitive> Cond, [1]: CNode_308, [2]: ValueNode<BoolImm> false}
#   5: @mindspore_nn_layer_normalization_BatchNorm2d_construct_138:CNode_310{[0]: ValueNode<Primitive> Switch, [1]: CNode_309, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_311, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_312}
#   6: @mindspore_nn_layer_normalization_BatchNorm2d_construct_138:CNode_313{[0]: CNode_310}
#   7: @mindspore_nn_layer_normalization_BatchNorm2d_construct_138:CNode_315{[0]: ValueNode<Primitive> Return, [1]: CNode_314}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_137 : 0000028E6F11DAD0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_137 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12](%para106_x) {
  %1(CNode_317) = call @mindspore_nn_layer_conv_Conv2d_construct_316()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv1-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv1-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_137:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_137:CNode_317{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_316}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_137:CNode_318{[0]: ValueNode<Primitive> Return, [1]: CNode_317}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_156 : 0000028E6F114B60
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_156 parent: [subgraph @decay_weight_113]() {
  Return(%para82_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:450/        return gradients/
}
# Order:
#   1: @decay_weight_156:CNode_319{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_159 : 0000028E6F110BA0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_159 parent: [subgraph @flatten_gradients_112]() {
  Return(%para83_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:427/        return gradients/
}
# Order:
#   1: @flatten_gradients_159:CNode_320{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_162 : 0000028E6F1120E0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_162 parent: [subgraph @gradients_centralization_127]() {
  %1(CNode_322) = call @gradients_centralization_321()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_162:CNode_322{[0]: ValueNode<FuncGraph> gradients_centralization_321}
#   2: @gradients_centralization_162:CNode_323{[0]: ValueNode<Primitive> Return, [1]: CNode_322}


subgraph attr:
subgraph instance: assign_166 : 0000028E6F11A060
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\parameter_func.py:24/def assign(variable, value):/
subgraph @assign_166(%para107_variable, %para108_value) {
  %1(CNode_324) = S_Prim_Assign[input_names: ["ref", "value"], output_names: ["output"], side_effect_mem: Bool(1)](%para107_variable, %para108_value)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\parameter_func.py:58/    return assign_(variable, value)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\parameter_func.py:58/    return assign_(variable, value)/
}
# Order:
#   1: @assign_166:CNode_324{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Assign, [1]: param_variable, [2]: param_value}
#   2: @assign_166:CNode_325{[0]: ValueNode<Primitive> Return, [1]: CNode_324}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_177 : 0000028E6F119B10
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_177(%para109_params, %para110_beta1_power, %para111_beta2_power, %para112_moment1, %para113_moment2, %para114_lr, %para115_gradients) {
  %1(CNode_327) = call @_apply_adam_326()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:817/        if self.use_offload:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:817/        if self.use_offload:/
}
# Order:
#   1: @_apply_adam_177:CNode_327{[0]: ValueNode<FuncGraph> _apply_adam_326}
#   2: @_apply_adam_177:CNode_328{[0]: ValueNode<Primitive> Return, [1]: CNode_327}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_176 : 0000028E6F118080
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_176(%para116_gradients) {
  %1(CNode_329) = S_Prim_not_equal("CPU", "CPU")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %2(CNode_330) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %3(CNode_331) = Switch(%2, @_grad_sparse_indices_deduplicate_332, @_grad_sparse_indices_deduplicate_333)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %4(CNode_334) = %3()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %5(CNode_335) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %6(CNode_336) = Switch(%5, @_grad_sparse_indices_deduplicate_337, @_grad_sparse_indices_deduplicate_338)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %7(CNode_339) = %6()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %8(CNode_341) = call @_grad_sparse_indices_deduplicate_340(%7)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:919/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  Return(%8)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_176:CNode_329{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: ValueNode<StringImm> CPU, [2]: ValueNode<StringImm> CPU}
#   2: @_grad_sparse_indices_deduplicate_176:CNode_330{[0]: ValueNode<Primitive> Cond, [1]: CNode_329, [2]: ValueNode<BoolImm> false}
#   3: @_grad_sparse_indices_deduplicate_176:CNode_331{[0]: ValueNode<Primitive> Switch, [1]: CNode_330, [2]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_332, [3]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_333}
#   4: @_grad_sparse_indices_deduplicate_176:CNode_334{[0]: CNode_331}
#   5: @_grad_sparse_indices_deduplicate_176:CNode_335{[0]: ValueNode<Primitive> Cond, [1]: CNode_334, [2]: ValueNode<BoolImm> false}
#   6: @_grad_sparse_indices_deduplicate_176:CNode_336{[0]: ValueNode<Primitive> Switch, [1]: CNode_335, [2]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_337, [3]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_338}
#   7: @_grad_sparse_indices_deduplicate_176:CNode_339{[0]: CNode_336}
#   8: @_grad_sparse_indices_deduplicate_176:CNode_341{[0]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_340, [1]: CNode_339}
#   9: @_grad_sparse_indices_deduplicate_176:CNode_342{[0]: ValueNode<Primitive> Return, [1]: CNode_341}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_175 : 0000028E6F110100
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_175(%para117_gradients) {
  %1(CNode_344) = call @scale_grad_343()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_175:CNode_344{[0]: ValueNode<FuncGraph> scale_grad_343}
#   2: @scale_grad_175:CNode_345{[0]: ValueNode<Primitive> Return, [1]: CNode_344}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_174 : 0000028E6F113620
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_174 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12]() {
  %1(CNode_347) = call @get_lr_346()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_174:CNode_347{[0]: ValueNode<FuncGraph> get_lr_346}
#   2: @get_lr_174:CNode_348{[0]: ValueNode<Primitive> Return, [1]: CNode_347}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_184 : 0000028E6F3524C0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_184 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117]() {
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[is_grad: Bool(0), sens: F32(1), input_names: ["features", "labels"], output_names: ["output"]](%para84_logits, %para85_labels)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:783/                return x/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_184:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_184:CNode_349{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_185 : 0000028E6F34BFD0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_185 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117]() {
  %1(CNode_351) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_185:CNode_351{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_185:CNode_352{[0]: ValueNode<Primitive> Return, [1]: CNode_351}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_190 : 0000028E6F341B20
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_190(%para118_x, %para119_, %para120_) {
  %1(x_shape) = S_Prim_Shape(%para118_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_76) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_77) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
  %4(CNode_78) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_79) = S_Prim_not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_80) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_81) = Switch(%6, @L_mindspore_nn_layer_basic_Dense_construct_353, @L_mindspore_nn_layer_basic_Dense_construct_354)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_84) = %7()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %9(CNode_85) = call @L_mindspore_nn_layer_basic_Dense_construct_355(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:106/        x = self.fc2(x)/
  %10(CNode_86) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:106/        x = self.fc2(x)/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_190:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_190:CNode_76{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_190:CNode_78{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_190:CNode_79{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_78, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_190:CNode_80{[0]: ValueNode<Primitive> Cond, [1]: CNode_79, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_190:CNode_81{[0]: ValueNode<Primitive> Switch, [1]: CNode_80, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_353, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_354}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_190:CNode_84{[0]: CNode_81}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_190:CNode_85{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_355, [1]: CNode_84}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_190:CNode_86{[0]: ValueNode<Primitive> Depend, [1]: CNode_85, [2]: CNode_77}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_190:CNode_87{[0]: ValueNode<Primitive> Return, [1]: CNode_86}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_200 : 0000028E6F351A20
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_200 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_153]() {
  Return(%para90_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:192/            return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_200:CNode_356{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_201 : 0000028E6F34EA50
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_201 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_153]() {
  %1(CNode_358) = call @mindspore_nn_layer_basic_Dropout_construct_357()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_201:CNode_358{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_357}
#   2: @mindspore_nn_layer_basic_Dropout_construct_201:CNode_359{[0]: ValueNode<Primitive> Return, [1]: CNode_358}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_195 : 0000028E6F34FA40
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_195 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_153]() {
  %1(CNode_192) = $(mindspore_nn_layer_basic_Dropout_construct_153):S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_195:CNode_360{[0]: ValueNode<Primitive> Return, [1]: CNode_192}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_196 : 0000028E6F34F4F0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_196() {
  %1(CNode_361) = S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_362) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_363) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_364, @mindspore_nn_layer_basic_Dropout_construct_365)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_366) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_196:CNode_361{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 1}
#   2: @mindspore_nn_layer_basic_Dropout_construct_196:CNode_362{[0]: ValueNode<Primitive> Cond, [1]: CNode_361, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_196:CNode_363{[0]: ValueNode<Primitive> Switch, [1]: CNode_362, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_364, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_365}
#   4: @mindspore_nn_layer_basic_Dropout_construct_196:CNode_366{[0]: CNode_363}
#   5: @mindspore_nn_layer_basic_Dropout_construct_196:CNode_367{[0]: ValueNode<Primitive> Return, [1]: CNode_366}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_210 : 0000028E6F1FF4A0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_210(%para121_axis, %para122_ndim) {
  %1(CNode_368) = S_Prim_negative(%para122_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_369) = S_Prim_less(%para121_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %3(CNode_370) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %4(CNode_371) = Switch(%3, @check_axis_valid_372, @check_axis_valid_373)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %5(CNode_374) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %6(CNode_375) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %7(CNode_376) = Switch(%6, @check_axis_valid_377, @check_axis_valid_378)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %8(CNode_379) = %7()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_210:CNode_368{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_210:CNode_369{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_368}
#   3: @check_axis_valid_210:CNode_370{[0]: ValueNode<Primitive> Cond, [1]: CNode_369, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_210:CNode_371{[0]: ValueNode<Primitive> Switch, [1]: CNode_370, [2]: ValueNode<FuncGraph> check_axis_valid_372, [3]: ValueNode<FuncGraph> check_axis_valid_373}
#   5: @check_axis_valid_210:CNode_374{[0]: CNode_371}
#   6: @check_axis_valid_210:CNode_375{[0]: ValueNode<Primitive> Cond, [1]: CNode_374, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_210:CNode_376{[0]: ValueNode<Primitive> Switch, [1]: CNode_375, [2]: ValueNode<FuncGraph> check_axis_valid_377, [3]: ValueNode<FuncGraph> check_axis_valid_378}
#   8: @check_axis_valid_210:CNode_379{[0]: CNode_376}
#   9: @check_axis_valid_210:CNode_380{[0]: ValueNode<Primitive> Return, [1]: CNode_379}


subgraph attr:
subgraph instance: rank_204 : 0000028E6F204450
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1540/def rank(input_x):/
subgraph @rank_204(%para123_input_x) {
  %1(CNode_381) = S_Prim_Rank(%para123_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1570/    return rank_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1570/    return rank_(input_x)/
}
# Order:
#   1: @rank_204:CNode_381{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input_x}
#   2: @rank_204:CNode_382{[0]: ValueNode<Primitive> Return, [1]: CNode_381}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_208 : 0000028E6F348560
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @mindspore_nn_layer_basic_Flatten_construct_208 parent: [subgraph @mindspore_nn_layer_basic_Flatten_construct_151]() {
  %1(x_rank) = $(mindspore_nn_layer_basic_Flatten_construct_151):call @rank_204(%para92_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:462/        x_rank = F.rank(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_208:CNode_383{[0]: ValueNode<Primitive> Return, [1]: x_rank}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_209 : 0000028E6F3405E0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @mindspore_nn_layer_basic_Flatten_construct_209() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_209:CNode_384{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: flatten_220 : 0000028E6F200F30
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_220(%para124_input, %para125_order, %para126_start_dim, %para127_end_dim) {
  %1(CNode_385) = S_Prim_isinstance(%para124_input, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1730/    if not isinstance(input, Tensor):/
  %2(CNode_386) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1730/    if not isinstance(input, Tensor):/
  %3(CNode_387) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1730/    if not isinstance(input, Tensor):/
  %4(CNode_388) = Switch(%3, @flatten_389, @flatten_390)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1730/    if not isinstance(input, Tensor):/
  %5(CNode_391) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1730/    if not isinstance(input, Tensor):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1730/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_220:CNode_385{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_input, [2]: ValueNode<ClassType> class 'mindspore.common.tensor.Tensor'}
#   2: @flatten_220:CNode_386{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_385}
#   3: @flatten_220:CNode_387{[0]: ValueNode<Primitive> Cond, [1]: CNode_386, [2]: ValueNode<BoolImm> false}
#   4: @flatten_220:CNode_388{[0]: ValueNode<Primitive> Switch, [1]: CNode_387, [2]: ValueNode<FuncGraph> flatten_389, [3]: ValueNode<FuncGraph> flatten_390}
#   5: @flatten_220:CNode_391{[0]: CNode_388}
#   6: @flatten_220:CNode_392{[0]: ValueNode<Primitive> Return, [1]: CNode_391}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_228 : 0000028E6F1FEF50
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_228 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_150]() {
  %1(CNode_393) = getattr(%para93_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_395) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_394(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:87/        x = self.maxpool4(x)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_228:CNode_393{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_228:x{[0]: CNode_393, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_228:CNode_396{[0]: ValueNode<Primitive> Return, [1]: CNode_395}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_228:CNode_395{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_394, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_229 : 0000028E6F1F7A70
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_229 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_150]() {
  %1(CNode_397) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_394(%para93_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:87/        x = self.maxpool4(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_229:CNode_398{[0]: ValueNode<Primitive> Return, [1]: CNode_397}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_229:CNode_397{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_394, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_240 : 0000028E6F1F8510
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_240 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_149]() {
  Return(%para94_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:192/            return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_240:CNode_399{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_241 : 0000028E6F1FC4D0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_241 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_149]() {
  %1(CNode_401) = call @mindspore_nn_layer_basic_Dropout_construct_400()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_241:CNode_401{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_400}
#   2: @mindspore_nn_layer_basic_Dropout_construct_241:CNode_402{[0]: ValueNode<Primitive> Return, [1]: CNode_401}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_235 : 0000028E6F1FBF80
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_235 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_149]() {
  %1(CNode_232) = $(mindspore_nn_layer_basic_Dropout_construct_149):S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_235:CNode_403{[0]: ValueNode<Primitive> Return, [1]: CNode_232}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_236 : 0000028E6F1F9500
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_236() {
  %1(CNode_404) = S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_405) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_406) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_407, @mindspore_nn_layer_basic_Dropout_construct_408)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_409) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_236:CNode_404{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 1}
#   2: @mindspore_nn_layer_basic_Dropout_construct_236:CNode_405{[0]: ValueNode<Primitive> Cond, [1]: CNode_404, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_236:CNode_406{[0]: ValueNode<Primitive> Switch, [1]: CNode_405, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_407, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_408}
#   4: @mindspore_nn_layer_basic_Dropout_construct_236:CNode_409{[0]: CNode_406}
#   5: @mindspore_nn_layer_basic_Dropout_construct_236:CNode_410{[0]: ValueNode<Primitive> Return, [1]: CNode_409}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_250 : 0000028E6F1FCA20
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_250 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_148]() {
  %1(CNode_412) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_411()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_250:CNode_412{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_411}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_250:CNode_413{[0]: ValueNode<Primitive> Return, [1]: CNode_412}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_251 : 0000028E6F1EE060
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_251 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_148]() {
  %1(CNode_415) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_414()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_415{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_414}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_416{[0]: ValueNode<Primitive> Return, [1]: CNode_415}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_255 : 0000028E6F1F4AA0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_255 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_147]() {
  %1(CNode_418) = call @mindspore_nn_layer_conv_Conv2d_construct_417()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv4-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv4-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_255:CNode_418{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_417}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_255:CNode_419{[0]: ValueNode<Primitive> Return, [1]: CNode_418}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_262 : 0000028E6F1ECB20
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_262 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_146]() {
  %1(CNode_420) = getattr(%para97_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_422) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_421(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:80/        x = self.maxpool3(x)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_262:CNode_420{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_262:x{[0]: CNode_420, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_262:CNode_423{[0]: ValueNode<Primitive> Return, [1]: CNode_422}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_262:CNode_422{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_421, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_263 : 0000028E6F1F0040
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_263 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_146]() {
  %1(CNode_424) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_421(%para97_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:80/        x = self.maxpool3(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_263:CNode_425{[0]: ValueNode<Primitive> Return, [1]: CNode_424}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_263:CNode_424{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_421, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266 : 0000028E6F129A10
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266(%para128_x, %para129_, %para130_, %para131_, %para132_) {
  %1(CNode_426) = S_Prim_Shape(%para128_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_427) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_428) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_429) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_430) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_431) = Switch(%5, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_432, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_433)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_434) = %6()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_435) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266:CNode_426{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266:CNode_427{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_426, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266:CNode_429{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266:CNode_430{[0]: ValueNode<Primitive> Cond, [1]: CNode_429, [2]: ValueNode<BoolImm> false}
#   5: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266:CNode_431{[0]: ValueNode<Primitive> Switch, [1]: CNode_430, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_432, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_433}
#   6: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266:CNode_434{[0]: CNode_431}
#   7: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266:CNode_268{[0]: ValueNode<Primitive> Return, [1]: CNode_435}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_269 : 0000028E6F1EC5D0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_269 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_144]() {
  %1(CNode_437) = call @mindspore_nn_layer_conv_Conv2d_construct_436()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv3-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv3-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_269:CNode_437{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_436}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_269:CNode_438{[0]: ValueNode<Primitive> Return, [1]: CNode_437}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_276 : 0000028E6F1EC080
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_276 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_143]() {
  %1(CNode_439) = getattr(%para100_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_441) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_440(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:74/        x = self.maxpool2(x)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_276:CNode_439{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_276:x{[0]: CNode_439, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_276:CNode_442{[0]: ValueNode<Primitive> Return, [1]: CNode_441}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_276:CNode_441{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_440, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_277 : 0000028E6F12E470
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_277 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_143]() {
  %1(CNode_443) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_440(%para100_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:74/        x = self.maxpool2(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_277:CNode_444{[0]: ValueNode<Primitive> Return, [1]: CNode_443}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_277:CNode_443{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_440, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_288 : 0000028E6F127F80
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_288 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_142]() {
  Return(%para101_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:192/            return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_288:CNode_445{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_289 : 0000028E6F127A30
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_289 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_142]() {
  %1(CNode_447) = call @mindspore_nn_layer_basic_Dropout_construct_446()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_289:CNode_447{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_446}
#   2: @mindspore_nn_layer_basic_Dropout_construct_289:CNode_448{[0]: ValueNode<Primitive> Return, [1]: CNode_447}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_283 : 0000028E6F12DF20
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_283 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_142]() {
  %1(CNode_280) = $(mindspore_nn_layer_basic_Dropout_construct_142):S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_283:CNode_449{[0]: ValueNode<Primitive> Return, [1]: CNode_280}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_284 : 0000028E6F12BF40
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_284() {
  %1(CNode_450) = S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_451) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_452) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_453, @mindspore_nn_layer_basic_Dropout_construct_454)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_455) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_284:CNode_450{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 1}
#   2: @mindspore_nn_layer_basic_Dropout_construct_284:CNode_451{[0]: ValueNode<Primitive> Cond, [1]: CNode_450, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_284:CNode_452{[0]: ValueNode<Primitive> Switch, [1]: CNode_451, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_453, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_454}
#   4: @mindspore_nn_layer_basic_Dropout_construct_284:CNode_455{[0]: CNode_452}
#   5: @mindspore_nn_layer_basic_Dropout_construct_284:CNode_456{[0]: ValueNode<Primitive> Return, [1]: CNode_455}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_294 : 0000028E6F128F70
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_294 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_140]() {
  %1(CNode_458) = call @mindspore_nn_layer_conv_Conv2d_construct_457()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv2-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv2-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_294:CNode_458{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_457}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_294:CNode_459{[0]: ValueNode<Primitive> Return, [1]: CNode_458}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_301 : 0000028E6F12C490
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_301 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_139]() {
  %1(CNode_460) = getattr(%para104_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_462) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_461(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:67/        x = self.maxpool1(x)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_301:CNode_460{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_301:x{[0]: CNode_460, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_301:CNode_463{[0]: ValueNode<Primitive> Return, [1]: CNode_462}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_301:CNode_462{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_461, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_302 : 0000028E6F121FE0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_302 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_139]() {
  %1(CNode_464) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_461(%para104_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:67/        x = self.maxpool1(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_302:CNode_465{[0]: ValueNode<Primitive> Return, [1]: CNode_464}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_302:CNode_464{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_461, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_311 : 0000028E6F121A90
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_311 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_138]() {
  %1(CNode_467) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_466()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_311:CNode_467{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_466}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_311:CNode_468{[0]: ValueNode<Primitive> Return, [1]: CNode_467}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_312 : 0000028E6F120000
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_312 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_138]() {
  %1(CNode_470) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_469()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_312:CNode_470{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_469}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_312:CNode_471{[0]: ValueNode<Primitive> Return, [1]: CNode_470}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_316 : 0000028E6F11A5B0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_316 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_137]() {
  %1(CNode_473) = call @mindspore_nn_layer_conv_Conv2d_construct_472()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv1-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv1-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_316:CNode_473{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_472}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_316:CNode_474{[0]: ValueNode<Primitive> Return, [1]: CNode_473}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_321 : 0000028E6F112B80
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_321 parent: [subgraph @gradients_centralization_127]() {
  Return(%para87_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:469/        return gradients/
}
# Order:
#   1: @gradients_centralization_321:CNode_475{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_326 : 0000028E6F11E570
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_326 parent: [subgraph @_apply_adam_177]() {
  %1(CNode_477) = call @_apply_adam_476()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_326:CNode_477{[0]: ValueNode<FuncGraph> _apply_adam_476}
#   2: @_apply_adam_326:CNode_478{[0]: ValueNode<Primitive> Return, [1]: CNode_477}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
after_block : 1
subgraph instance: _grad_sparse_indices_deduplicate_340 : 0000028E6F1150B0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_340(%para133_) {
  Return(%para133_phi_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:521/        return gradients/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_340:CNode_479{[0]: ValueNode<Primitive> Return, [1]: param_phi_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_337 : 0000028E6F117090
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_337 parent: [subgraph @_grad_sparse_indices_deduplicate_176]() {
  %1(CNode_480) = S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_indices_deduplicate)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
  %2(gradients) = S_Prim_map(%1, %para116_gradients)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
  Return(%2)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_337:CNode_480{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_indices_deduplicate}
#   2: @_grad_sparse_indices_deduplicate_337:gradients{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_480, [2]: param_gradients}
#   3: @_grad_sparse_indices_deduplicate_337:CNode_481{[0]: ValueNode<Primitive> Return, [1]: gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_338 : 0000028E6F114610
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_338 parent: [subgraph @_grad_sparse_indices_deduplicate_176]() {
  Return(%para116_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_338:CNode_482{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_332 : 0000028E6F116B40
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_332() {
  Return(Bool(1))
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_332:CNode_483{[0]: ValueNode<Primitive> Return, [1]: ValueNode<BoolImm> true}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_333 : 0000028E6F1185D0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_333 parent: [subgraph @_grad_sparse_indices_deduplicate_176]() {
  %1(CNode_329) = $(_grad_sparse_indices_deduplicate_176):S_Prim_not_equal("CPU", "CPU")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_333:CNode_484{[0]: ValueNode<Primitive> Return, [1]: CNode_329}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_343 : 0000028E6F1140C0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_343 parent: [subgraph @scale_grad_175]() {
  %1(CNode_486) = call @scale_grad_485()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_343:CNode_486{[0]: ValueNode<FuncGraph> scale_grad_485}
#   2: @scale_grad_343:CNode_487{[0]: ValueNode<Primitive> Return, [1]: CNode_486}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_346 : 0000028E6F118B20
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_346 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12]() {
  %1(CNode_489) = call @get_lr_488()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_346:CNode_489{[0]: ValueNode<FuncGraph> get_lr_488}
#   2: @get_lr_346:CNode_490{[0]: ValueNode<Primitive> Return, [1]: CNode_489}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350 : 0000028E6F34DFB0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_117]() {
  %1(CNode_492) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_491()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350:CNode_493{[0]: ValueNode<FuncGraph> shape_494, [1]: param_logits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350:CNode_495{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350:CNode_496{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_493, [2]: CNode_495}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350:labels{[0]: ValueNode<DoSignaturePrimitive> S_Prim_OneHot, [1]: param_labels, [2]: CNode_496, [3]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1), [4]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0)}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350:CNode_492{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_491}
#   6: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350:CNode_497{[0]: ValueNode<Primitive> Return, [1]: CNode_492}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_355 : 0000028E6F3415D0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_355 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_190](%para134_) {
  %1(CNode_88) = call @L_mindspore_nn_layer_basic_Dense_construct_498()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_355:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_phi_x, [2]: param_L_fc2.weight}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_355:CNode_88{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_498}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_355:CNode_89{[0]: ValueNode<Primitive> Return, [1]: CNode_88}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_353 : 0000028E6F349000
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_353 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_190]() {
  %1(CNode_499) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %2(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_190):S_Prim_Shape(%para118_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %3(CNode_500) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %4(CNode_501) = S_Prim_getitem(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %5(CNode_502) = S_Prim_MakeTuple(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %6(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para118_x, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_353:CNode_499{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_353:CNode_500{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_353:CNode_501{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_500}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_353:CNode_502{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_499, [2]: CNode_501}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_353:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_x, [2]: CNode_502}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_353:CNode_503{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_354 : 0000028E6F348AB0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_354 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_190]() {
  Return(%para118_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_354:CNode_504{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_357 : 0000028E6F3534B0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_357 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_153]() {
  %1(CNode_505) = S_Prim_Dropout[keep_prob: F32(0.8), Seed0: I64(0), Seed1: I64(0), side_effect_hidden: Bool(1)](%para90_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  %2(out) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:195/        return out/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_357:CNode_505{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Dropout, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Dropout_construct_357:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_505, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Dropout_construct_357:CNode_506{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_364 : 0000028E6F34CA70
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_364 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_196]() {
  %1(CNode_361) = $(mindspore_nn_layer_basic_Dropout_construct_196):S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_364:CNode_507{[0]: ValueNode<Primitive> Return, [1]: CNode_361}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_365 : 0000028E6F34DA60
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_365() {
  %1(CNode_508) = S_Prim_equal(F32(0.2), I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout4-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_365:CNode_508{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.2, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_basic_Dropout_construct_365:CNode_509{[0]: ValueNode<Primitive> Return, [1]: CNode_508}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_377 : 0000028E6F200490
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_377() {
  %1(CNode_510) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @check_axis_valid_377:CNode_510{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @check_axis_valid_377:CNode_511{[0]: ValueNode<Primitive> Return, [1]: CNode_510}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_378 : 0000028E6F1FF9F0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_378() {
  %1(CNode_513) = call @check_axis_valid_512()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_378:CNode_513{[0]: ValueNode<FuncGraph> check_axis_valid_512}
#   2: @check_axis_valid_378:CNode_514{[0]: ValueNode<Primitive> Return, [1]: CNode_513}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_372 : 0000028E6F1F8FB0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_372 parent: [subgraph @check_axis_valid_210]() {
  %1(CNode_368) = $(check_axis_valid_210):S_Prim_negative(%para122_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_369) = $(check_axis_valid_210):S_Prim_less(%para121_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_372:CNode_515{[0]: ValueNode<Primitive> Return, [1]: CNode_369}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_373 : 0000028E6F2009E0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_373 parent: [subgraph @check_axis_valid_210]() {
  %1(CNode_516) = S_Prim_greater_equal(%para121_axis, %para122_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_373:CNode_516{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @check_axis_valid_373:CNode_517{[0]: ValueNode<Primitive> Return, [1]: CNode_516}


subgraph attr:
subgraph instance: flatten_389 : 0000028E6F348010
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_389() {
  %1(CNode_518) = JoinedStr("For 'flatten', argument 'input' must be Tensor.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1731/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  %2(CNode_519) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1731/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1731/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
}
# Order:
#   1: @flatten_389:CNode_518{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', argument 'input' must be Tensor.}
#   2: @flatten_389:CNode_519{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_518, [3]: ValueNode<StringImm> None}
#   3: @flatten_389:CNode_520{[0]: ValueNode<Primitive> Return, [1]: CNode_519}


subgraph attr:
subgraph instance: flatten_390 : 0000028E6F205440
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_390 parent: [subgraph @flatten_220]() {
  %1(CNode_522) = call @flatten_521()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1730/    if not isinstance(input, Tensor):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1730/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_390:CNode_522{[0]: ValueNode<FuncGraph> flatten_521}
#   2: @flatten_390:CNode_523{[0]: ValueNode<Primitive> Return, [1]: CNode_522}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_394 : 0000028E6F1F8A60
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_394(%para135_, %para136_) {
  %1(CNode_525) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_524()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_394:CNode_525{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_524}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_394:CNode_526{[0]: ValueNode<Primitive> Return, [1]: CNode_525}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_400 : 0000028E6F1F9FA0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_400 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_149]() {
  %1(CNode_527) = S_Prim_Dropout[keep_prob: F32(0.8), Seed0: I64(0), Seed1: I64(0), side_effect_hidden: Bool(1)](%para94_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  %2(out) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:195/        return out/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_400:CNode_527{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Dropout, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Dropout_construct_400:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_527, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Dropout_construct_400:CNode_528{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_407 : 0000028E6F1F9A50
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_407 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_236]() {
  %1(CNode_404) = $(mindspore_nn_layer_basic_Dropout_construct_236):S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_407:CNode_529{[0]: ValueNode<Primitive> Return, [1]: CNode_404}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_408 : 0000028E6F1F7FC0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_408() {
  %1(CNode_530) = S_Prim_equal(F32(0.2), I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout2-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_408:CNode_530{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.2, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_basic_Dropout_construct_408:CNode_531{[0]: ValueNode<Primitive> Return, [1]: CNode_530}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_411 : 0000028E6F1FCF70
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_411 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_148]() {
  %1(CNode_532) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para95_x, %para13_bn4.gamma, %para14_bn4.beta, %para55_bn4.moving_mean, %para56_bn4.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_533) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_411:CNode_532{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_bn4.gamma, [3]: param_bn4.beta, [4]: param_bn4.moving_mean, [5]: param_bn4.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_411:CNode_533{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_532, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_411:CNode_534{[0]: ValueNode<Primitive> Return, [1]: CNode_533}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_414 : 0000028E6F1F5A90
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_414 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_148]() {
  %1(CNode_535) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_536) = Switch(%1, @mindspore_nn_layer_normalization_BatchNorm2d_construct_537, @mindspore_nn_layer_normalization_BatchNorm2d_construct_538)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_539) = %2()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_414:CNode_535{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_414:CNode_536{[0]: ValueNode<Primitive> Switch, [1]: CNode_535, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_537, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_538}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_414:CNode_539{[0]: CNode_536}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_414:CNode_540{[0]: ValueNode<Primitive> Return, [1]: CNode_539}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_417 : 0000028E6F1F4FF0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_417 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_147]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_147):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1), pad_list: (I64(1), I64(1), I64(1), I64(1))](%para96_x, %para12_conv4.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:conv4.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv4-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv4-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_417:CNode_541{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_421 : 0000028E6F1F3560
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_421(%para137_, %para138_) {
  %1(CNode_543) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_542()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_421:CNode_543{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_542}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_421:CNode_544{[0]: ValueNode<Primitive> Return, [1]: CNode_543}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_432 : 0000028E6F12AF50
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_432 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266]() {
  %1(CNode_546) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_545()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_432:CNode_546{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_545}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_432:CNode_547{[0]: ValueNode<Primitive> Return, [1]: CNode_546}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_433 : 0000028E6F129F60
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_433 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266]() {
  %1(CNode_549) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_548()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_433:CNode_549{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_548}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_433:CNode_550{[0]: ValueNode<Primitive> Return, [1]: CNode_549}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_436 : 0000028E6F1EF050
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_436 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_144]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_144):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1), pad_list: (I64(1), I64(1), I64(1), I64(1))](%para99_x, %para9_conv3.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 64, 3, 3), ref_key=:conv3.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv3-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv3-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_436:CNode_551{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_440 : 0000028E6F126A40
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_440(%para139_, %para140_) {
  %1(CNode_553) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_552()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_440:CNode_553{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_552}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_440:CNode_554{[0]: ValueNode<Primitive> Return, [1]: CNode_553}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_446 : 0000028E6F1264F0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_446 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_142]() {
  %1(CNode_555) = S_Prim_Dropout[keep_prob: F32(0.8), Seed0: I64(0), Seed1: I64(0), side_effect_hidden: Bool(1)](%para101_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  %2(out) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:195/        return out/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_446:CNode_555{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Dropout, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Dropout_construct_446:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_555, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Dropout_construct_446:CNode_556{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_453 : 0000028E6F125A50
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_453 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_284]() {
  %1(CNode_450) = $(mindspore_nn_layer_basic_Dropout_construct_284):S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_453:CNode_557{[0]: ValueNode<Primitive> Return, [1]: CNode_450}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_454 : 0000028E6F12D480
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_454() {
  %1(CNode_558) = S_Prim_equal(F32(0.2), I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/dropout1-Dropout)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_454:CNode_558{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.2, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_basic_Dropout_construct_454:CNode_559{[0]: ValueNode<Primitive> Return, [1]: CNode_558}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_457 : 0000028E6F12B9F0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_457 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_140]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_140):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1), pad_list: (I64(1), I64(1), I64(1), I64(1))](%para103_x, %para6_conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv2-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv2-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_457:CNode_560{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_461 : 0000028E6F122A80
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_461(%para141_, %para142_) {
  %1(CNode_562) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_561()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_461:CNode_562{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_561}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_461:CNode_563{[0]: ValueNode<Primitive> Return, [1]: CNode_562}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_466 : 0000028E6F122530
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_466 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_138]() {
  %1(CNode_564) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para105_x, %para4_bn1.gamma, %para5_bn1.beta, %para61_bn1.moving_mean, %para62_bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_565) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_466:CNode_564{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_bn1.gamma, [3]: param_bn1.beta, [4]: param_bn1.moving_mean, [5]: param_bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_466:CNode_565{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_564, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_466:CNode_566{[0]: ValueNode<Primitive> Return, [1]: CNode_565}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_469 : 0000028E6F120FF0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_469 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_138]() {
  %1(CNode_567) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_568) = Switch(%1, @mindspore_nn_layer_normalization_BatchNorm2d_construct_569, @mindspore_nn_layer_normalization_BatchNorm2d_construct_570)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_571) = %2()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_469:CNode_567{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_469:CNode_568{[0]: ValueNode<Primitive> Switch, [1]: CNode_567, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_569, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_570}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_469:CNode_571{[0]: CNode_568}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_469:CNode_572{[0]: ValueNode<Primitive> Return, [1]: CNode_571}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_472 : 0000028E6F11F010
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_472 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_137]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_137):S_Prim_Conv2D[out_channel: I64(32), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1), pad_list: (I64(1), I64(1), I64(1), I64(1))](%para106_x, %para3_conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv1-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/conv1-Conv2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_472:CNode_573{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_476 : 0000028E6F11CAE0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_476 parent: [subgraph @_apply_adam_177]() {
  %1(CNode_575) = call @_apply_adam_574()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_476:CNode_575{[0]: ValueNode<FuncGraph> _apply_adam_574}
#   2: @_apply_adam_476:CNode_576{[0]: ValueNode<Primitive> Return, [1]: CNode_575}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_485 : 0000028E6F119070
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_485 parent: [subgraph @scale_grad_175]() {
  Return(%para117_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:488/        return gradients/
}
# Order:
#   1: @scale_grad_485:CNode_577{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_488 : 0000028E6F111640
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_488 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12]() {
  Return(%para54_learning_rate)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\optimizer.py:756/        return lr/
}
# Order:
#   1: @get_lr_488:CNode_578{[0]: ValueNode<Primitive> Return, [1]: param_learning_rate}


subgraph attr:
subgraph instance: shape_494 : 0000028E6F34FF90
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1483/def shape(input_x):/
subgraph @shape_494(%para143_input_x) {
  %1(CNode_579) = S_Prim_Shape(%para143_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1509/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1509/    return shape_(input_x)/
}
# Order:
#   1: @shape_494:CNode_579{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @shape_494:CNode_580{[0]: ValueNode<Primitive> Return, [1]: CNode_579}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_491 : 0000028E6F3504E0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_491 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350]() {
  %1(CNode_493) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350):call @shape_494(%para84_logits)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %2(CNode_495) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350):S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %3(CNode_496) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350):S_Prim_getitem(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %4(labels) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_350):S_Prim_OneHot[axis: I64(-1), input_names: ["indices", "depth", "on_value", "off_value"], output_names: ["output"]](%para85_labels, %3, Tensor(shape=[], dtype=Float32, value=1), Tensor(shape=[], dtype=Float32, value=0))
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %5(CNode_581) = S_Prim_SoftmaxCrossEntropyWithLogits(%para84_logits, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %6(x) = S_Prim_getitem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %7(CNode_583) = call @get_loss_582(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_491:CNode_581{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_491:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_581, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_491:CNode_583{[0]: ValueNode<FuncGraph> get_loss_582, [1]: x}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_491:CNode_584{[0]: ValueNode<Primitive> Return, [1]: CNode_583}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_498 : 0000028E6F349550
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_498 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_355]() {
  %1(CNode_90) = call @L_mindspore_nn_layer_basic_Dense_construct_585()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_498:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc2.bias}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_498:CNode_90{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_585}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_498:CNode_91{[0]: ValueNode<Primitive> Return, [1]: CNode_90}


subgraph attr:
training : 1
after_block : 1
subgraph instance: check_axis_valid_512 : 0000028E6F1FFF40
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_512() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
}
# Order:
#   1: @check_axis_valid_512:CNode_586{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: flatten_521 : 0000028E6F205990
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_521 parent: [subgraph @flatten_220]() {
  %1(CNode_587) = S_Prim_isinstance(%para126_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_588) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_589) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %4(CNode_590) = Switch(%3, @flatten_591, @flatten_592)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %5(CNode_593) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %6(CNode_594) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %7(CNode_595) = Switch(%6, @flatten_596, @flatten_597)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %8(CNode_598) = %7()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_521:CNode_587{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @flatten_521:CNode_588{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_587}
#   3: @flatten_521:CNode_589{[0]: ValueNode<Primitive> Cond, [1]: CNode_588, [2]: ValueNode<BoolImm> false}
#   4: @flatten_521:CNode_590{[0]: ValueNode<Primitive> Switch, [1]: CNode_589, [2]: ValueNode<FuncGraph> flatten_591, [3]: ValueNode<FuncGraph> flatten_592}
#   5: @flatten_521:CNode_593{[0]: CNode_590}
#   6: @flatten_521:CNode_594{[0]: ValueNode<Primitive> Cond, [1]: CNode_593, [2]: ValueNode<BoolImm> false}
#   7: @flatten_521:CNode_595{[0]: ValueNode<Primitive> Switch, [1]: CNode_594, [2]: ValueNode<FuncGraph> flatten_596, [3]: ValueNode<FuncGraph> flatten_597}
#   8: @flatten_521:CNode_598{[0]: CNode_595}
#   9: @flatten_521:CNode_599{[0]: ValueNode<Primitive> Return, [1]: CNode_598}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_524 : 0000028E6F1FA4F0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_524 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_394]() {
  %1(CNode_601) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_600()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_524:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_524:CNode_601{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_600}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_524:CNode_602{[0]: ValueNode<Primitive> Return, [1]: CNode_601}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_537 : 0000028E6F1F6FD0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_537 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_148]() {
  %1(CNode_603) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para95_x, %para13_bn4.gamma, %para14_bn4.beta, %para55_bn4.moving_mean, %para56_bn4.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_604) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_537:CNode_603{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_bn4.gamma, [3]: param_bn4.beta, [4]: param_bn4.moving_mean, [5]: param_bn4.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_537:CNode_604{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_603, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_537:CNode_605{[0]: ValueNode<Primitive> Return, [1]: CNode_604}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_538 : 0000028E6F1F5FE0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_538 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_148]() {
  %1(CNode_607) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_606()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_538:CNode_607{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_606}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_538:CNode_608{[0]: ValueNode<Primitive> Return, [1]: CNode_607}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_542 : 0000028E6F1F0AE0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_542 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_421]() {
  %1(CNode_610) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_609()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_542:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_542:CNode_610{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_609}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_542:CNode_611{[0]: ValueNode<Primitive> Return, [1]: CNode_610}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_545 : 0000028E6F12B4A0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_545 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266]() {
  %1(CNode_612) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para128_x, %para129_L_bn3.gamma, %para130_L_bn3.beta, %para131_L_bn3.moving_mean, %para132_L_bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_613) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_545:CNode_612{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_bn3.gamma, [3]: param_L_bn3.beta, [4]: param_L_bn3.moving_mean, [5]: param_L_bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_545:CNode_613{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_612, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_545:CNode_614{[0]: ValueNode<Primitive> Return, [1]: CNode_613}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_548 : 0000028E6F125500
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_548 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266]() {
  %1(CNode_615) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_616) = Switch(%1, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_617, @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_618)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_619) = %2()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_548:CNode_615{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_548:CNode_616{[0]: ValueNode<Primitive> Switch, [1]: CNode_615, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_617, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_618}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_548:CNode_619{[0]: CNode_616}
#   4: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_548:CNode_620{[0]: ValueNode<Primitive> Return, [1]: CNode_619}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_552 : 0000028E6F12E9C0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_552 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_440]() {
  %1(CNode_622) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_621()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_552:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_552:CNode_622{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_621}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_552:CNode_623{[0]: ValueNode<Primitive> Return, [1]: CNode_622}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_561 : 0000028E6F122FD0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_561 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_461]() {
  %1(CNode_625) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_624()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_561:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_561:CNode_625{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_624}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_561:CNode_626{[0]: ValueNode<Primitive> Return, [1]: CNode_625}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_569 : 0000028E6F121540
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_569 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_138]() {
  %1(CNode_627) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para105_x, %para4_bn1.gamma, %para5_bn1.beta, %para61_bn1.moving_mean, %para62_bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_628) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_569:CNode_627{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_bn1.gamma, [3]: param_bn1.beta, [4]: param_bn1.moving_mean, [5]: param_bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_569:CNode_628{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_627, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_569:CNode_629{[0]: ValueNode<Primitive> Return, [1]: CNode_628}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_570 : 0000028E6F11B5A0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_570 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_138]() {
  %1(CNode_631) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_630()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_570:CNode_631{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_630}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_570:CNode_632{[0]: ValueNode<Primitive> Return, [1]: CNode_631}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_574 : 0000028E6F11D030
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_574 parent: [subgraph @_apply_adam_177]() {
  %1(CNode_634) = call @_apply_adam_633()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_574:CNode_634{[0]: ValueNode<FuncGraph> _apply_adam_633}
#   2: @_apply_adam_574:CNode_635{[0]: ValueNode<Primitive> Return, [1]: CNode_634}


subgraph attr:
training : 1
subgraph instance: get_loss_582 : 0000028E6F353A00
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_582(%para144_x, %para145_weights) {
  %1(CNode_637) = call @get_loss_636()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
}
# Order:
#   1: @get_loss_582:input_dtype{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> dtype}
#   2: @get_loss_582:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_x, [2]: ValueNode<Float> Float32}
#   3: @get_loss_582:weights{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_weights, [2]: ValueNode<Float> Float32}
#   4: @get_loss_582:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Mul, [1]: weights, [2]: x}
#   5: @get_loss_582:CNode_637{[0]: ValueNode<FuncGraph> get_loss_636}
#   6: @get_loss_582:CNode_638{[0]: ValueNode<Primitive> Return, [1]: CNode_637}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_585 : 0000028E6F349AA0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_585 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_498]() {
  %1(CNode_92) = call @L_mindspore_nn_layer_basic_Dense_construct_639()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_585:CNode_92{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_639}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_585:CNode_93{[0]: ValueNode<Primitive> Return, [1]: CNode_92}


subgraph attr:
subgraph instance: flatten_596 : 0000028E6F345590
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_596() {
  %1(CNode_640) = JoinedStr("For 'flatten', both 'start_dim' and 'end_dim' must be int.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1734/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  %2(CNode_641) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1734/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1734/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
}
# Order:
#   1: @flatten_596:CNode_640{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', both 'start_dim' and 'end_dim' must be int.}
#   2: @flatten_596:CNode_641{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_640, [3]: ValueNode<StringImm> None}
#   3: @flatten_596:CNode_642{[0]: ValueNode<Primitive> Return, [1]: CNode_641}


subgraph attr:
subgraph instance: flatten_597 : 0000028E6F2029C0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_597 parent: [subgraph @flatten_220]() {
  %1(CNode_644) = call @flatten_643()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_597:CNode_644{[0]: ValueNode<FuncGraph> flatten_643}
#   2: @flatten_597:CNode_645{[0]: ValueNode<Primitive> Return, [1]: CNode_644}


subgraph attr:
subgraph instance: flatten_591 : 0000028E6F205EE0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_591 parent: [subgraph @flatten_521]() {
  %1(CNode_587) = $(flatten_521):S_Prim_isinstance(%para126_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_588) = $(flatten_521):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_591:CNode_646{[0]: ValueNode<Primitive> Return, [1]: CNode_588}


subgraph attr:
subgraph instance: flatten_592 : 0000028E6F207970
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_592 parent: [subgraph @flatten_220]() {
  %1(CNode_647) = S_Prim_isinstance(%para127_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_648) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_649) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_650) = Switch(%3, @flatten_651, @flatten_652)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %5(CNode_653) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_592:CNode_647{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @flatten_592:CNode_648{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_647}
#   3: @flatten_592:CNode_649{[0]: ValueNode<Primitive> Cond, [1]: CNode_648, [2]: ValueNode<BoolImm> false}
#   4: @flatten_592:CNode_650{[0]: ValueNode<Primitive> Switch, [1]: CNode_649, [2]: ValueNode<FuncGraph> flatten_651, [3]: ValueNode<FuncGraph> flatten_652}
#   5: @flatten_592:CNode_653{[0]: CNode_650}
#   6: @flatten_592:CNode_654{[0]: ValueNode<Primitive> Return, [1]: CNode_653}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_600 : 0000028E6F1FAA40
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_600 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_524]() {
  %1(CNode_655) = Cond(%para136_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_656) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_657, @mindspore_nn_layer_pooling_MaxPool2d_construct_658)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_659) = %2()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_661) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_660(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:87/        x = self.maxpool4(x)/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_600:CNode_655{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_600:CNode_656{[0]: ValueNode<Primitive> Switch, [1]: CNode_655, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_657, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_658}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_600:CNode_659{[0]: CNode_656}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_600:CNode_661{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_660, [1]: CNode_659}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_600:CNode_662{[0]: ValueNode<Primitive> Return, [1]: CNode_661}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_606 : 0000028E6F1EE5B0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_606 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_148]() {
  %1(CNode_663) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para95_x, %para13_bn4.gamma, %para14_bn4.beta, %para55_bn4.moving_mean, %para56_bn4.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.beta>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:bn4.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_664) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn4-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_606:CNode_663{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_bn4.gamma, [3]: param_bn4.beta, [4]: param_bn4.moving_mean, [5]: param_bn4.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_606:CNode_664{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_663, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_606:CNode_665{[0]: ValueNode<Primitive> Return, [1]: CNode_664}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_609 : 0000028E6F1F1580
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_609 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_542]() {
  %1(CNode_666) = Cond(%para138_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_667) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_668, @mindspore_nn_layer_pooling_MaxPool2d_construct_669)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_670) = %2()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_672) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_671(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:80/        x = self.maxpool3(x)/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_609:CNode_666{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_609:CNode_667{[0]: ValueNode<Primitive> Switch, [1]: CNode_666, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_668, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_669}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_609:CNode_670{[0]: CNode_667}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_609:CNode_672{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_671, [1]: CNode_670}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_609:CNode_673{[0]: ValueNode<Primitive> Return, [1]: CNode_672}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_617 : 0000028E6F12AA00
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_617 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266]() {
  %1(CNode_674) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para128_x, %para129_L_bn3.gamma, %para130_L_bn3.beta, %para131_L_bn3.moving_mean, %para132_L_bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_675) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_617:CNode_674{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_bn3.gamma, [3]: param_L_bn3.beta, [4]: param_L_bn3.moving_mean, [5]: param_L_bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_617:CNode_675{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_674, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_617:CNode_676{[0]: ValueNode<Primitive> Return, [1]: CNode_675}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_618 : 0000028E6F12A4B0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_618 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266]() {
  %1(CNode_678) = call @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_677()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_618:CNode_678{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_normalization_BatchNorm2d_construct_677}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_618:CNode_679{[0]: ValueNode<Primitive> Return, [1]: CNode_678}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_621 : 0000028E6F126F90
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_621 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_552]() {
  %1(CNode_680) = Cond(%para140_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_681) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_682, @mindspore_nn_layer_pooling_MaxPool2d_construct_683)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_684) = %2()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_686) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_685(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:74/        x = self.maxpool2(x)/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_621:CNode_680{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_621:CNode_681{[0]: ValueNode<Primitive> Switch, [1]: CNode_680, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_682, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_683}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_621:CNode_684{[0]: CNode_681}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_621:CNode_686{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_685, [1]: CNode_684}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_621:CNode_687{[0]: ValueNode<Primitive> Return, [1]: CNode_686}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_624 : 0000028E6F123520
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_624 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_561]() {
  %1(CNode_688) = Cond(%para142_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_689) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_690, @mindspore_nn_layer_pooling_MaxPool2d_construct_691)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_692) = %2()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_694) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_693(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:67/        x = self.maxpool1(x)/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_624:CNode_688{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_624:CNode_689{[0]: ValueNode<Primitive> Switch, [1]: CNode_688, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_690, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_691}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_624:CNode_692{[0]: CNode_689}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_624:CNode_694{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_693, [1]: CNode_692}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_624:CNode_695{[0]: ValueNode<Primitive> Return, [1]: CNode_694}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_630 : 0000028E6F1195C0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_630 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_138]() {
  %1(CNode_696) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para105_x, %para4_bn1.gamma, %para5_bn1.beta, %para61_bn1.moving_mean, %para62_bn1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:bn1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_697) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn1-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_630:CNode_696{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_bn1.gamma, [3]: param_bn1.beta, [4]: param_bn1.moving_mean, [5]: param_bn1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_630:CNode_697{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_696, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_630:CNode_698{[0]: ValueNode<Primitive> Return, [1]: CNode_697}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_633 : 0000028E6F11D580
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_633 parent: [subgraph @_apply_adam_177]() {
  %1(CNode_700) = call @_apply_adam_699()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_633:CNode_700{[0]: ValueNode<FuncGraph> _apply_adam_699}
#   2: @_apply_adam_633:CNode_701{[0]: ValueNode<Primitive> Return, [1]: CNode_700}


subgraph attr:
training : 1
subgraph instance: get_loss_636 : 0000028E6F34EFA0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_636 parent: [subgraph @get_loss_582]() {
  %1(CNode_703) = call @get_loss_702()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
}
# Order:
#   1: @get_loss_636:CNode_704{[0]: ValueNode<FuncGraph> get_axis_705, [1]: x}
#   2: @get_loss_636:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: x, [2]: CNode_704}
#   3: @get_loss_636:CNode_703{[0]: ValueNode<FuncGraph> get_loss_702}
#   4: @get_loss_636:CNode_706{[0]: ValueNode<Primitive> Return, [1]: CNode_703}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_639 : 0000028E6F349FF0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_639 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_498]() {
  %1(CNode_94) = call @L_mindspore_nn_layer_basic_Dense_construct_707()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_639:CNode_94{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_707}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_639:CNode_95{[0]: ValueNode<Primitive> Return, [1]: CNode_94}


subgraph attr:
after_block : 1
subgraph instance: flatten_643 : 0000028E6F2039B0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_643 parent: [subgraph @flatten_220]() {
  %1(CNode_708) = S_Prim_check_flatten_order[constexpr_prim: Bool(1)](%para125_order)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1735/    check_flatten_order_const(order)/
  %2(CNode_709) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %3(CNode_710) = S_Prim_equal(%para125_order, "F")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1736/    if order == 'F':/
  %4(CNode_711) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1736/    if order == 'F':/
  %5(CNode_712) = Switch(%4, @flatten_713, @flatten_714)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1736/    if order == 'F':/
  %6(CNode_715) = %5()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1736/    if order == 'F':/
  %7(CNode_717) = call @flatten_716(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %8(CNode_718) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1736/    if order == 'F':/
}
# Order:
#   1: @flatten_643:CNode_708{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_flatten_order, [1]: param_order}
#   2: @flatten_643:CNode_710{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_order, [2]: ValueNode<StringImm> F}
#   3: @flatten_643:CNode_711{[0]: ValueNode<Primitive> Cond, [1]: CNode_710, [2]: ValueNode<BoolImm> false}
#   4: @flatten_643:CNode_712{[0]: ValueNode<Primitive> Switch, [1]: CNode_711, [2]: ValueNode<FuncGraph> flatten_713, [3]: ValueNode<FuncGraph> flatten_714}
#   5: @flatten_643:CNode_715{[0]: CNode_712}
#   6: @flatten_643:CNode_717{[0]: ValueNode<FuncGraph> flatten_716, [1]: CNode_715}
#   7: @flatten_643:CNode_718{[0]: ValueNode<Primitive> Depend, [1]: CNode_717, [2]: CNode_709}
#   8: @flatten_643:CNode_719{[0]: ValueNode<Primitive> Return, [1]: CNode_718}


subgraph attr:
subgraph instance: flatten_651 : 0000028E6F202F10
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_651 parent: [subgraph @flatten_592]() {
  %1(CNode_647) = $(flatten_592):S_Prim_isinstance(%para127_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_648) = $(flatten_592):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @flatten_651:CNode_720{[0]: ValueNode<Primitive> Return, [1]: CNode_648}


subgraph attr:
subgraph instance: flatten_652 : 0000028E6F202470
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_652 parent: [subgraph @flatten_220]() {
  %1(CNode_721) = S_Prim_isinstance(%para126_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %2(CNode_722) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %3(CNode_723) = Switch(%2, @flatten_724, @flatten_725)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_726) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_652:CNode_721{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @flatten_652:CNode_722{[0]: ValueNode<Primitive> Cond, [1]: CNode_721, [2]: ValueNode<BoolImm> false}
#   3: @flatten_652:CNode_723{[0]: ValueNode<Primitive> Switch, [1]: CNode_722, [2]: ValueNode<FuncGraph> flatten_724, [3]: ValueNode<FuncGraph> flatten_725}
#   4: @flatten_652:CNode_726{[0]: CNode_723}
#   5: @flatten_652:CNode_727{[0]: ValueNode<Primitive> Return, [1]: CNode_726}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_660 : 0000028E6F1F6530
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_660(%para146_) {
  %1(CNode_729) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_728()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_660:CNode_729{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_728}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_660:CNode_730{[0]: ValueNode<Primitive> Return, [1]: CNode_729}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_657 : 0000028E6F1F6A80
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_657 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_524]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_524):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para135_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_731) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_732) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_733) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_734, @mindspore_nn_layer_pooling_MaxPool2d_construct_735)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_736) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_738) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_737(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:87/        x = self.maxpool4(x)/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_657:CNode_731{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_657:CNode_732{[0]: ValueNode<Primitive> Cond, [1]: CNode_731, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_657:CNode_733{[0]: ValueNode<Primitive> Switch, [1]: CNode_732, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_734, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_735}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_657:CNode_736{[0]: CNode_733}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_657:CNode_738{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_737, [1]: CNode_736}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_657:CNode_739{[0]: ValueNode<Primitive> Return, [1]: CNode_738}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_658 : 0000028E6F1FDA10
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_658 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_524]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_524):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para135_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_658:CNode_740{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_671 : 0000028E6F1F4550
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_671(%para147_) {
  %1(CNode_742) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_741()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_671:CNode_742{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_741}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_671:CNode_743{[0]: ValueNode<Primitive> Return, [1]: CNode_742}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_668 : 0000028E6F1EFAF0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_668 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_542]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_542):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para137_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_744) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_745) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_746) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_747, @mindspore_nn_layer_pooling_MaxPool2d_construct_748)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_749) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_751) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_750(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:80/        x = self.maxpool3(x)/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_668:CNode_744{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_668:CNode_745{[0]: ValueNode<Primitive> Cond, [1]: CNode_744, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_668:CNode_746{[0]: ValueNode<Primitive> Switch, [1]: CNode_745, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_747, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_748}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_668:CNode_749{[0]: CNode_746}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_668:CNode_751{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_750, [1]: CNode_749}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_668:CNode_752{[0]: ValueNode<Primitive> Return, [1]: CNode_751}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_669 : 0000028E6F1F1AD0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_669 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_542]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_542):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para137_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_669:CNode_753{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_normalization_BatchNorm2d_construct_677 : 0000028E6F12C9E0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_677 parent: [subgraph @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_266]() {
  %1(CNode_754) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para128_x, %para129_L_bn3.gamma, %para130_L_bn3.beta, %para131_L_bn3.moving_mean, %para132_L_bn3.moving_variance)
      : (<null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_755) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/bn3-BatchNorm2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_677:CNode_754{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_L_bn3.gamma, [3]: param_L_bn3.beta, [4]: param_L_bn3.moving_mean, [5]: param_L_bn3.moving_variance}
#   2: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_677:CNode_755{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_754, [2]: ValueNode<Int64Imm> 0}
#   3: @L_mindspore_nn_layer_normalization_BatchNorm2d_construct_677:CNode_756{[0]: ValueNode<Primitive> Return, [1]: CNode_755}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_685 : 0000028E6F1EEB00
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_685(%para148_) {
  %1(CNode_758) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_757()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_685:CNode_758{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_757}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_685:CNode_759{[0]: ValueNode<Primitive> Return, [1]: CNode_758}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_682 : 0000028E6F1274E0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_682 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_552]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_552):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para139_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_760) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_761) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_762) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_763, @mindspore_nn_layer_pooling_MaxPool2d_construct_764)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_765) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_767) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_766(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:74/        x = self.maxpool2(x)/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_682:CNode_760{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_682:CNode_761{[0]: ValueNode<Primitive> Cond, [1]: CNode_760, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_682:CNode_762{[0]: ValueNode<Primitive> Switch, [1]: CNode_761, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_763, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_764}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_682:CNode_765{[0]: CNode_762}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_682:CNode_767{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_766, [1]: CNode_765}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_682:CNode_768{[0]: ValueNode<Primitive> Return, [1]: CNode_767}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_683 : 0000028E6F12EF10
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_683 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_552]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_552):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para139_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_683:CNode_769{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_693 : 0000028E6F124FB0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_693(%para149_) {
  %1(CNode_771) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_770()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_693:CNode_771{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_770}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_693:CNode_772{[0]: ValueNode<Primitive> Return, [1]: CNode_771}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_690 : 0000028E6F11C040
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_690 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_561]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_561):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para141_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_773) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_774) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_775) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_776, @mindspore_nn_layer_pooling_MaxPool2d_construct_777)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_778) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_780) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_779(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:67/        x = self.maxpool1(x)/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_690:CNode_773{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_690:CNode_774{[0]: ValueNode<Primitive> Cond, [1]: CNode_773, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_690:CNode_775{[0]: ValueNode<Primitive> Switch, [1]: CNode_774, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_776, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_777}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_690:CNode_778{[0]: CNode_775}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_690:CNode_780{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_779, [1]: CNode_778}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_690:CNode_781{[0]: ValueNode<Primitive> Return, [1]: CNode_780}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_691 : 0000028E6F123A70
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_691 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_561]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_561):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para141_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_691:CNode_782{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_699 : 0000028E6F120550
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_699 parent: [subgraph @_apply_adam_177]() {
  %1(CNode_784) = call @_apply_adam_783()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
}
# Order:
#   1: @_apply_adam_699:CNode_785{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_adam_opt, [2]: ValueNode<DoSignaturePrimitive> S_Prim_Adam, [3]: ValueNode<DoSignaturePrimitive> S_Prim_FusedSparseAdam, [4]: ValueNode<DoSignaturePrimitive> S_Prim_Push, [5]: ValueNode<DoSignaturePrimitive> S_Prim_Pull, [6]: ValueNode<BoolImm> false, [7]: ValueNode<BoolImm> false, [8]: ValueNode<BoolImm> true, [9]: param_beta1_power, [10]: param_beta2_power, [11]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.9), [12]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.999), [13]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1e-08), [14]: param_lr}
#   2: @_apply_adam_699:success{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_785, [2]: param_gradients, [3]: param_params, [4]: param_moment1, [5]: param_moment2, [6]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false), [7]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false)}
#   3: @_apply_adam_699:CNode_784{[0]: ValueNode<FuncGraph> _apply_adam_783}
#   4: @_apply_adam_699:CNode_786{[0]: ValueNode<Primitive> Return, [1]: CNode_784}


subgraph attr:
training : 1
subgraph instance: get_axis_705 : 0000028E6F3544A0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:113/    def get_axis(self, x):/
subgraph @get_axis_705(%para150_x) {
  %1(shape) = call @shape_494(%para150_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:120/        shape = F.shape(x)/
  %2(length) = S_Prim_sequence_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:121/        length = F.tuple_len(shape)/
  %3(perm) = S_Prim_make_range(I64(0), %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:122/        perm = F.make_range(0, length)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:123/        return perm/
}
# Order:
#   1: @get_axis_705:shape{[0]: ValueNode<FuncGraph> shape_494, [1]: param_x}
#   2: @get_axis_705:length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_sequence_len, [1]: shape}
#   3: @get_axis_705:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: length}
#   4: @get_axis_705:CNode_787{[0]: ValueNode<Primitive> Return, [1]: perm}


subgraph attr:
training : 1
subgraph instance: get_loss_702 : 0000028E6F351F70
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_702 parent: [subgraph @get_loss_636]() {
  %1(CNode_789) = call @get_loss_788()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_702:CNode_789{[0]: ValueNode<FuncGraph> get_loss_788}
#   2: @get_loss_702:CNode_790{[0]: ValueNode<Primitive> Return, [1]: CNode_789}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_707 : 0000028E6F34A540
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_707 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_498]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_190):S_Prim_Shape(%para118_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_96) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_97) = S_Prim_not_equal(%2, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_98) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_99) = Switch(%4, @L_mindspore_nn_layer_basic_Dense_construct_791, @L_mindspore_nn_layer_basic_Dense_construct_792)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %6(CNode_101) = %5()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_103) = call @L_mindspore_nn_layer_basic_Dense_construct_793(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet)
      # In file D:\pythonstarter\dshwnotbroken\mindspore\mynet.py:106/        x = self.fc2(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_707:CNode_96{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_707:CNode_97{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_96, [2]: ValueNode<Int64Imm> 2}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_707:CNode_98{[0]: ValueNode<Primitive> Cond, [1]: CNode_97, [2]: ValueNode<BoolImm> false}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_707:CNode_99{[0]: ValueNode<Primitive> Switch, [1]: CNode_98, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_791, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_792}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_707:CNode_101{[0]: CNode_99}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_707:CNode_103{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_793, [1]: CNode_101}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_707:CNode_104{[0]: ValueNode<Primitive> Return, [1]: CNode_103}


subgraph attr:
after_block : 1
subgraph instance: flatten_716 : 0000028E6F208960
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_716 parent: [subgraph @flatten_220](%para151_) {
  %1(CNode_794) = S_Prim_equal(%para126_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_795) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  %3(CNode_796) = Switch(%2, @flatten_797, @flatten_798)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  %4(CNode_799) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  %5(CNode_800) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  %6(CNode_801) = Switch(%5, @flatten_802, @flatten_803)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  %7(CNode_804) = %6()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_716:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_phi_input}
#   2: @flatten_716:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_phi_input}
#   3: @flatten_716:CNode_794{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_start_dim, [2]: ValueNode<Int64Imm> 1}
#   4: @flatten_716:CNode_795{[0]: ValueNode<Primitive> Cond, [1]: CNode_794, [2]: ValueNode<BoolImm> false}
#   5: @flatten_716:CNode_796{[0]: ValueNode<Primitive> Switch, [1]: CNode_795, [2]: ValueNode<FuncGraph> flatten_797, [3]: ValueNode<FuncGraph> flatten_798}
#   6: @flatten_716:CNode_799{[0]: CNode_796}
#   7: @flatten_716:CNode_800{[0]: ValueNode<Primitive> Cond, [1]: CNode_799, [2]: ValueNode<BoolImm> false}
#   8: @flatten_716:CNode_801{[0]: ValueNode<Primitive> Switch, [1]: CNode_800, [2]: ValueNode<FuncGraph> flatten_802, [3]: ValueNode<FuncGraph> flatten_803}
#   9: @flatten_716:CNode_804{[0]: CNode_801}
#  10: @flatten_716:CNode_805{[0]: ValueNode<Primitive> Return, [1]: CNode_804}


subgraph attr:
subgraph instance: flatten_713 : 0000028E6F203F00
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_713 parent: [subgraph @flatten_220]() {
  %1(CNode_807) = call @_get_cache_prim_806(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1739/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %2(CNode_808) = %1()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1739/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %3(CNode_809) = call @rank_204(%para124_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1737/        perm = ops.make_range(0, ops.rank(input))/
  %4(perm) = S_Prim_make_range(I64(0), %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1737/        perm = ops.make_range(0, ops.rank(input))/
  %5(new_order) = S_Prim_tuple_reversed(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1738/        new_order = ops.tuple_reversed(perm)/
  %6(input) = %2(%para124_input, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1739/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1737/        perm = ops.make_range(0, ops.rank(input))/
}
# Order:
#   1: @flatten_713:CNode_809{[0]: ValueNode<FuncGraph> rank_204, [1]: param_input}
#   2: @flatten_713:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: CNode_809}
#   3: @flatten_713:new_order{[0]: ValueNode<DoSignaturePrimitive> S_Prim_tuple_reversed, [1]: perm}
#   4: @flatten_713:CNode_807{[0]: ValueNode<FuncGraph> _get_cache_prim_806, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.Transpose'}
#   5: @flatten_713:CNode_808{[0]: CNode_807}
#   6: @flatten_713:input{[0]: CNode_808, [1]: param_input, [2]: new_order}
#   7: @flatten_713:CNode_810{[0]: ValueNode<Primitive> Return, [1]: input}


subgraph attr:
subgraph instance: flatten_714 : 0000028E6F206430
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_714 parent: [subgraph @flatten_220]() {
  Return(%para124_input)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1736/    if order == 'F':/
}
# Order:
#   1: @flatten_714:CNode_811{[0]: ValueNode<Primitive> Return, [1]: param_input}


subgraph attr:
subgraph instance: flatten_724 : 0000028E6F203460
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_724 parent: [subgraph @flatten_652]() {
  %1(CNode_721) = $(flatten_652):S_Prim_isinstance(%para126_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @flatten_724:CNode_812{[0]: ValueNode<Primitive> Return, [1]: CNode_721}


subgraph attr:
subgraph instance: flatten_725 : 0000028E6F208410
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_725 parent: [subgraph @flatten_220]() {
  %1(CNode_813) = S_Prim_isinstance(%para127_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1733/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1732/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_725:CNode_813{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @flatten_725:CNode_814{[0]: ValueNode<Primitive> Return, [1]: CNode_813}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_728 : 0000028E6F1FE4B0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_728 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_660]() {
  %1(CNode_816) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_815()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_728:CNode_816{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_815}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_728:CNode_817{[0]: ValueNode<Primitive> Return, [1]: CNode_816}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_737 : 0000028E6F1FDF60
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_737(%para152_) {
  Return(%para152_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_737:CNode_818{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_734 : 0000028E6F1FBA30
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_734 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_524]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_524):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para135_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_819) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_820) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_821) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_822) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_823) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_824) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_734:CNode_819{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_734:CNode_820{[0]: ValueNode<Primitive> getattr, [1]: CNode_819, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_734:CNode_821{[0]: CNode_820, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_734:CNode_822{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_734:CNode_823{[0]: ValueNode<Primitive> getattr, [1]: CNode_822, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_734:CNode_824{[0]: CNode_823, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_734:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_821, [2]: CNode_824}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_734:CNode_825{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_735 : 0000028E6F1FAF90
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_735 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_524]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_524):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para135_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_826) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_735:CNode_826{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_735:out{[0]: CNode_826, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_735:CNode_827{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_741 : 0000028E6F1F2020
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_741 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_671]() {
  %1(CNode_829) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_828()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_741:CNode_829{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_828}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_741:CNode_830{[0]: ValueNode<Primitive> Return, [1]: CNode_829}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_750 : 0000028E6F1F4000
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_750(%para153_) {
  Return(%para153_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_750:CNode_831{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_747 : 0000028E6F1EDB10
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_747 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_542]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_542):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para137_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_832) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_833) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_834) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_835) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_836) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_837) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_747:CNode_832{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_747:CNode_833{[0]: ValueNode<Primitive> getattr, [1]: CNode_832, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_747:CNode_834{[0]: CNode_833, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_747:CNode_835{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_747:CNode_836{[0]: ValueNode<Primitive> getattr, [1]: CNode_835, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_747:CNode_837{[0]: CNode_836, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_747:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_834, [2]: CNode_837}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_747:CNode_838{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_748 : 0000028E6F1ED5C0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_748 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_542]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_542):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para137_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_839) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_748:CNode_839{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_748:out{[0]: CNode_839, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_748:CNode_840{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_757 : 0000028E6F1F3AB0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_757 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_685]() {
  %1(CNode_842) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_841()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_757:CNode_842{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_841}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_757:CNode_843{[0]: ValueNode<Primitive> Return, [1]: CNode_842}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_766 : 0000028E6F1F0590
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_766(%para154_) {
  Return(%para154_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_766:CNode_844{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_763 : 0000028E6F12F9B0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_763 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_552]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_552):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para139_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_845) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_846) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_847) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_848) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_849) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_850) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_763:CNode_845{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_763:CNode_846{[0]: ValueNode<Primitive> getattr, [1]: CNode_845, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_763:CNode_847{[0]: CNode_846, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_763:CNode_848{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_763:CNode_849{[0]: ValueNode<Primitive> getattr, [1]: CNode_848, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_763:CNode_850{[0]: CNode_849, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_763:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_847, [2]: CNode_850}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_763:CNode_851{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_764 : 0000028E6F12F460
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_764 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_552]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_552):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para139_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_852) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_764:CNode_852{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_764:out{[0]: CNode_852, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_764:CNode_853{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_770 : 0000028E6F1284D0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_770 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_693]() {
  %1(CNode_855) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_854()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_770:CNode_855{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_854}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_770:CNode_856{[0]: ValueNode<Primitive> Return, [1]: CNode_855}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_779 : 0000028E6F124510
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_779(%para155_) {
  Return(%para155_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_779:CNode_857{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_776 : 0000028E6F124A60
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_776 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_561]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_561):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para141_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_858) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_859) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_860) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_861) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_862) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_863) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_776:CNode_858{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_776:CNode_859{[0]: ValueNode<Primitive> getattr, [1]: CNode_858, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_776:CNode_860{[0]: CNode_859, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_776:CNode_861{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_776:CNode_862{[0]: ValueNode<Primitive> getattr, [1]: CNode_861, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_776:CNode_863{[0]: CNode_862, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_776:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_860, [2]: CNode_863}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_776:CNode_864{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_777 : 0000028E6F123FC0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_777 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_561]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_561):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(1), format: "NCHW", output_names: ["output"]](%para141_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_865) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_777:CNode_865{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_777:out{[0]: CNode_865, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_777:CNode_866{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_783 : 0000028E6F11AB00
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_783 parent: [subgraph @_apply_adam_699]() {
  %1(CNode_868) = call @_apply_adam_867()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_783:CNode_868{[0]: ValueNode<FuncGraph> _apply_adam_867}
#   2: @_apply_adam_783:CNode_869{[0]: ValueNode<Primitive> Return, [1]: CNode_868}


subgraph attr:
training : 1
subgraph instance: get_loss_788 : 0000028E6F350A30
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_788 parent: [subgraph @get_loss_636]() {
  %1(CNode_871) = call @get_loss_870()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_788:CNode_871{[0]: ValueNode<FuncGraph> get_loss_870}
#   2: @get_loss_788:CNode_872{[0]: ValueNode<Primitive> Return, [1]: CNode_871}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_793 : 0000028E6F3435B0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_793(%para156_) {
  Return(%para156_phi_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:635/        return x/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_793:CNode_873{[0]: ValueNode<Primitive> Return, [1]: param_phi_x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_791 : 0000028E6F34AA90
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_791 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_498]() {
  %1(x) = $(L_mindspore_nn_layer_basic_Dense_construct_355):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para134_phi_x, %para120_L_fc2.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_mindspore_nn_layer_basic_Dense_construct_498):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"]](%1, %para119_L_fc2.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  %3(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_190):S_Prim_Shape(%para118_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %4(CNode_874) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %5(CNode_875) = S_Prim_make_slice(None, %4, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %6(CNode_876) = S_Prim_getitem(%3, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %7(CNode_878) = call @L_shape_877(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %8(CNode_879) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %9(CNode_880) = S_Prim_getitem(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %10(CNode_881) = S_Prim_MakeTuple(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %11(out_shape) = S_Prim_add(%6, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %12(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%2, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:634/            x = self.reshape(x, out_shape)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_791:CNode_874{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_791:CNode_875{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: CNode_874, [3]: ValueNode<None> None}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_791:CNode_876{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_875}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_791:CNode_878{[0]: ValueNode<FuncGraph> L_shape_877, [1]: x}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_791:CNode_879{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_791:CNode_880{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_878, [2]: CNode_879}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_791:CNode_881{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_880}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_791:out_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_876, [2]: CNode_881}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_791:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: x, [2]: out_shape}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_791:CNode_882{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_792 : 0000028E6F343060
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_792 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_498]() {
  %1(x) = $(L_mindspore_nn_layer_basic_Dense_construct_355):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para134_phi_x, %para120_L_fc2.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_mindspore_nn_layer_basic_Dense_construct_498):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"]](%1, %para119_L_fc2.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/fc2-Dense)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_792:CNode_105{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: flatten_802 : 0000028E6F347AC0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_802 parent: [subgraph @flatten_716]() {
  %1(x_rank) = $(flatten_716):S_Prim_Rank(%para151_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1743/    x_rank = rank_(input)/
  %2(CNode_883) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1745/        if x_rank in (0, 1):/
  %3(CNode_884) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1745/        if x_rank in (0, 1):/
  %4(CNode_885) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1745/        if x_rank in (0, 1):/
  %5(CNode_886) = Switch(%4, @flatten_887, @flatten_888)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1745/        if x_rank in (0, 1):/
  %6(CNode_889) = %5()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1745/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1745/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_802:CNode_883{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   2: @flatten_802:CNode_884{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_883}
#   3: @flatten_802:CNode_885{[0]: ValueNode<Primitive> Cond, [1]: CNode_884, [2]: ValueNode<BoolImm> false}
#   4: @flatten_802:CNode_886{[0]: ValueNode<Primitive> Switch, [1]: CNode_885, [2]: ValueNode<FuncGraph> flatten_887, [3]: ValueNode<FuncGraph> flatten_888}
#   5: @flatten_802:CNode_889{[0]: CNode_886}
#   6: @flatten_802:CNode_890{[0]: ValueNode<Primitive> Return, [1]: CNode_889}


subgraph attr:
subgraph instance: flatten_803 : 0000028E6F209400
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_803 parent: [subgraph @flatten_716]() {
  %1(CNode_892) = call @flatten_891()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_803:CNode_892{[0]: ValueNode<FuncGraph> flatten_891}
#   2: @flatten_803:CNode_893{[0]: ValueNode<Primitive> Return, [1]: CNode_892}


subgraph attr:
subgraph instance: flatten_797 : 0000028E6F201480
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_797 parent: [subgraph @flatten_220]() {
  %1(CNode_894) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_895) = S_Prim_equal(%para127_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_797:CNode_894{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_797:CNode_895{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_end_dim, [2]: CNode_894}
#   3: @flatten_797:CNode_896{[0]: ValueNode<Primitive> Return, [1]: CNode_895}


subgraph attr:
subgraph instance: flatten_798 : 0000028E6F208EB0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_798 parent: [subgraph @flatten_716]() {
  %1(CNode_794) = $(flatten_716):S_Prim_equal(%para126_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1744/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_798:CNode_897{[0]: ValueNode<Primitive> Return, [1]: CNode_794}


subgraph attr:
subgraph instance: _get_cache_prim_806 : 0000028E6F206980
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_806(%para157_cls) {
  %1(CNode_899) = call @_get_cache_prim_898()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
}
# Order:
#   1: @_get_cache_prim_806:CNode_899{[0]: ValueNode<FuncGraph> _get_cache_prim_898}
#   2: @_get_cache_prim_806:CNode_900{[0]: ValueNode<Primitive> Return, [1]: CNode_899}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_815 : 0000028E6F1FEA00
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_815 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_660]() {
  Return(%para146_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool4-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_815:CNode_901{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_828 : 0000028E6F1F2570
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_828 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_671]() {
  Return(%para147_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool3-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_828:CNode_902{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_841 : 0000028E6F1ED070
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_841 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_685]() {
  Return(%para148_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool2-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_841:CNode_903{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_854 : 0000028E6F128A20
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_854 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_693]() {
  Return(%para149_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/maxpool1-MaxPool2d)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_854:CNode_904{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_867 : 0000028E6F11B050
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_867 parent: [subgraph @_apply_adam_699]() {
  %1(CNode_906) = call @_apply_adam_905()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_867:CNode_906{[0]: ValueNode<FuncGraph> _apply_adam_905}
#   2: @_apply_adam_867:CNode_907{[0]: ValueNode<Primitive> Return, [1]: CNode_906}


subgraph attr:
training : 1
subgraph instance: get_loss_870 : 0000028E6F350F80
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_870 parent: [subgraph @get_loss_636]() {
  %1(weights) = $(get_loss_582):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para145_weights, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:141/        weights = self.cast(weights, mstype.float32)/
  %2(x) = $(get_loss_582):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para144_x, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:140/        x = self.cast(x, mstype.float32)/
  %3(x) = $(get_loss_582):S_Prim_Mul[input_names: ["x", "y"], output_names: ["output"]](%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:142/        x = self.mul(weights, x)/
  %4(CNode_704) = $(get_loss_636):call @get_axis_705(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %5(x) = $(get_loss_636):S_Prim_ReduceMean[keep_dims: Bool(0), input_names: ["input_x", "axis"], output_names: ["y"]](%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %6(input_dtype) = $(get_loss_582):getattr(%para144_x, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:139/        input_dtype = x.dtype/
  %7(x) = S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:147/        x = self.cast(x, input_dtype)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\loss\loss.py:148/        return x/
}
# Order:
#   1: @get_loss_870:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: x, [2]: input_dtype}
#   2: @get_loss_870:CNode_908{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: L_shape_877 : 0000028E6F34AFE0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1483/def shape(input_x):/
subgraph @L_shape_877(%para158_input_x) {
  %1(CNode_579) = S_Prim_Shape(%para158_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1509/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1509/    return shape_(input_x)/
}
# Order:
#   1: @L_shape_877:CNode_579{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @L_shape_877:CNode_580{[0]: ValueNode<Primitive> Return, [1]: CNode_579}


subgraph attr:
subgraph instance: flatten_887 : 0000028E6F345040
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_887 parent: [subgraph @flatten_716]() {
  %1(CNode_909) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1746/            return reshape_(input, (-1,))/
  %2(CNode_910) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1746/            return reshape_(input, (-1,))/
  %3(CNode_911) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para151_phi_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1746/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1746/            return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_887:CNode_909{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_887:CNode_910{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_909}
#   3: @flatten_887:CNode_911{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: CNode_910}
#   4: @flatten_887:CNode_912{[0]: ValueNode<Primitive> Return, [1]: CNode_911}


subgraph attr:
subgraph instance: flatten_888 : 0000028E6F340090
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_888 parent: [subgraph @flatten_716]() {
  %1(CNode_914) = call @flatten_913()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1745/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1745/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_888:CNode_914{[0]: ValueNode<FuncGraph> flatten_913}
#   2: @flatten_888:CNode_915{[0]: ValueNode<Primitive> Return, [1]: CNode_914}


subgraph attr:
after_block : 1
subgraph instance: flatten_891 : 0000028E6F204EF0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_891 parent: [subgraph @flatten_716]() {
  %1(x_rank) = $(flatten_716):S_Prim_Rank(%para151_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1743/    x_rank = rank_(input)/
  %2(idx) = call @canonicalize_axis_916(%para126_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1750/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = call @canonicalize_axis_916(%para127_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1751/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_918) = call @check_dim_valid_917(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1752/    check_dim_valid(start_dim, end_dim)/
  %5(CNode_919) = StopGradient(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %6(CNode_920) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1754/    if x_rank in (0, 1):/
  %7(CNode_921) = S_Prim_in(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1754/    if x_rank in (0, 1):/
  %8(CNode_922) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1754/    if x_rank in (0, 1):/
  %9(CNode_923) = Switch(%8, @flatten_924, @flatten_925)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1754/    if x_rank in (0, 1):/
  %10(CNode_926) = %9()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1754/    if x_rank in (0, 1):/
  %11(CNode_927) = Depend[side_effect_propagate: I64(1)](%10, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1754/    if x_rank in (0, 1):/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1754/    if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_891:idx{[0]: ValueNode<FuncGraph> canonicalize_axis_916, [1]: param_start_dim, [2]: x_rank}
#   2: @flatten_891:end_dim{[0]: ValueNode<FuncGraph> canonicalize_axis_916, [1]: param_end_dim, [2]: x_rank}
#   3: @flatten_891:CNode_918{[0]: ValueNode<FuncGraph> check_dim_valid_917, [1]: idx, [2]: end_dim}
#   4: @flatten_891:CNode_920{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   5: @flatten_891:CNode_921{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_920}
#   6: @flatten_891:CNode_922{[0]: ValueNode<Primitive> Cond, [1]: CNode_921, [2]: ValueNode<BoolImm> false}
#   7: @flatten_891:CNode_923{[0]: ValueNode<Primitive> Switch, [1]: CNode_922, [2]: ValueNode<FuncGraph> flatten_924, [3]: ValueNode<FuncGraph> flatten_925}
#   8: @flatten_891:CNode_926{[0]: CNode_923}
#   9: @flatten_891:CNode_928{[0]: ValueNode<Primitive> Return, [1]: CNode_927}


subgraph attr:
subgraph instance: _get_cache_prim_898 : 0000028E6F207420
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_898 parent: [subgraph @_get_cache_prim_806]() {
  Return(@_new_prim_for_graph_929)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\_primitive_cache.py:89/        return _new_prim_for_graph/
}
# Order:
#   1: @_get_cache_prim_898:CNode_930{[0]: ValueNode<Primitive> Return, [1]: ValueNode<FuncGraph> _new_prim_for_graph_929}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_905 : 0000028E6F120AA0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_905 parent: [subgraph @_apply_adam_699]() {
  %1(CNode_932) = call @_apply_adam_931()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_905:CNode_932{[0]: ValueNode<FuncGraph> _apply_adam_931}
#   2: @_apply_adam_905:CNode_933{[0]: ValueNode<Primitive> Return, [1]: CNode_932}


subgraph attr:
after_block : 1
subgraph instance: flatten_913 : 0000028E6F340B30
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_913 parent: [subgraph @flatten_716]() {
  %1(CNode_934) = call @_get_cache_prim_806(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1747/        return _get_cache_prim(P.Flatten)()(input)/
  %2(CNode_935) = %1()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1747/        return _get_cache_prim(P.Flatten)()(input)/
  %3(CNode_936) = %2(%para151_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1747/        return _get_cache_prim(P.Flatten)()(input)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1747/        return _get_cache_prim(P.Flatten)()(input)/
}
# Order:
#   1: @flatten_913:CNode_934{[0]: ValueNode<FuncGraph> _get_cache_prim_806, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.nn_ops.Flatten'}
#   2: @flatten_913:CNode_935{[0]: CNode_934}
#   3: @flatten_913:CNode_936{[0]: CNode_935, [1]: param_phi_input}
#   4: @flatten_913:CNode_937{[0]: ValueNode<Primitive> Return, [1]: CNode_936}


subgraph attr:
subgraph instance: check_dim_valid_917 : 0000028E6F342070
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1720/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_917(%para159_start_dim, %para160_end_dim) {
  %1(CNode_938) = S_Prim_greater(%para159_start_dim, %para160_end_dim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1721/        if start_dim > end_dim:/
  %2(CNode_939) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1721/        if start_dim > end_dim:/
  %3(CNode_940) = Switch(%2, @check_dim_valid_941, @check_dim_valid_942)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1721/        if start_dim > end_dim:/
  %4(CNode_943) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1721/        if start_dim > end_dim:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1721/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_917:CNode_938{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater, [1]: param_start_dim, [2]: param_end_dim}
#   2: @check_dim_valid_917:CNode_939{[0]: ValueNode<Primitive> Cond, [1]: CNode_938, [2]: ValueNode<BoolImm> false}
#   3: @check_dim_valid_917:CNode_940{[0]: ValueNode<Primitive> Switch, [1]: CNode_939, [2]: ValueNode<FuncGraph> check_dim_valid_941, [3]: ValueNode<FuncGraph> check_dim_valid_942}
#   4: @check_dim_valid_917:CNode_943{[0]: CNode_940}
#   5: @check_dim_valid_917:CNode_944{[0]: ValueNode<Primitive> Return, [1]: CNode_943}


subgraph attr:
subgraph instance: canonicalize_axis_916 : 0000028E6F20AE90
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1724/    def canonicalize_axis(axis, x_rank):/
subgraph @canonicalize_axis_916(%para161_axis, %para162_x_rank) {
  %1(CNode_945) = S_Prim_not_equal(%para162_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_946) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_947) = Switch(%2, @canonicalize_axis_948, @canonicalize_axis_949)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = %3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_951) = call @check_axis_valid_950(%para161_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1726/        check_axis_valid(axis, ndim)/
  %6(CNode_952) = StopGradient(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1724/    def canonicalize_axis(axis, x_rank):/
  %7(CNode_953) = S_Prim_greater_equal(%para161_axis, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
  %8(CNode_954) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
  %9(CNode_955) = Switch(%8, @canonicalize_axis_956, @canonicalize_axis_957)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
  %10(CNode_958) = %9()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
  %11(CNode_959) = Depend[side_effect_propagate: I64(1)](%10, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_916:CNode_945{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: param_x_rank, [2]: ValueNode<Int64Imm> 0}
#   2: @canonicalize_axis_916:CNode_946{[0]: ValueNode<Primitive> Cond, [1]: CNode_945, [2]: ValueNode<BoolImm> false}
#   3: @canonicalize_axis_916:CNode_947{[0]: ValueNode<Primitive> Switch, [1]: CNode_946, [2]: ValueNode<FuncGraph> canonicalize_axis_948, [3]: ValueNode<FuncGraph> canonicalize_axis_949}
#   4: @canonicalize_axis_916:ndim{[0]: CNode_947}
#   5: @canonicalize_axis_916:CNode_951{[0]: ValueNode<FuncGraph> check_axis_valid_950, [1]: param_axis, [2]: ndim}
#   6: @canonicalize_axis_916:CNode_953{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: ValueNode<Int64Imm> 0}
#   7: @canonicalize_axis_916:CNode_954{[0]: ValueNode<Primitive> Cond, [1]: CNode_953, [2]: ValueNode<BoolImm> false}
#   8: @canonicalize_axis_916:CNode_955{[0]: ValueNode<Primitive> Switch, [1]: CNode_954, [2]: ValueNode<FuncGraph> canonicalize_axis_956, [3]: ValueNode<FuncGraph> canonicalize_axis_957}
#   9: @canonicalize_axis_916:CNode_958{[0]: CNode_955}
#  10: @canonicalize_axis_916:CNode_960{[0]: ValueNode<Primitive> Return, [1]: CNode_959}


subgraph attr:
subgraph instance: flatten_924 : 0000028E6F20B930
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_924 parent: [subgraph @flatten_716]() {
  %1(CNode_961) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1755/        return reshape_(input, (-1,))/
  %2(CNode_962) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1755/        return reshape_(input, (-1,))/
  %3(CNode_963) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para151_phi_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1755/        return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1755/        return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_924:CNode_961{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_924:CNode_962{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_961}
#   3: @flatten_924:CNode_963{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: CNode_962}
#   4: @flatten_924:CNode_964{[0]: ValueNode<Primitive> Return, [1]: CNode_963}


subgraph attr:
subgraph instance: flatten_925 : 0000028E6F201F20
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_925 parent: [subgraph @flatten_891]() {
  %1(CNode_966) = call @flatten_965()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1754/    if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1754/    if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_925:CNode_966{[0]: ValueNode<FuncGraph> flatten_965}
#   2: @flatten_925:CNode_967{[0]: ValueNode<Primitive> Return, [1]: CNode_966}


subgraph attr:
subgraph instance: _new_prim_for_graph_929 : 0000028E6F206ED0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\_primitive_cache.py:67/    def _new_prim_for_graph(*args, **kwargs) -> Primitive:/
subgraph @_new_prim_for_graph_929 parent: [subgraph @_get_cache_prim_806](%para163_args, %para164_kwargs) {
  %1(CNode_968) = UnpackCall_unpack_call(%para157_cls, %para163_args, %para164_kwargs)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\_primitive_cache.py:68/        return cls(*args, **kwargs)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\_primitive_cache.py:68/        return cls(*args, **kwargs)/
}
# Order:
#   1: @_new_prim_for_graph_929:CNode_968{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.969, [1]: param_cls, [2]: param_args, [3]: param_kwargs}
#   2: @_new_prim_for_graph_929:CNode_970{[0]: ValueNode<Primitive> Return, [1]: CNode_968}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_931 : 0000028E6F11F560
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_931 parent: [subgraph @_apply_adam_699]() {
  %1(CNode_972) = call @_apply_adam_971()
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_931:CNode_972{[0]: ValueNode<FuncGraph> _apply_adam_971}
#   2: @_apply_adam_931:CNode_973{[0]: ValueNode<Primitive> Return, [1]: CNode_972}


subgraph attr:
subgraph instance: check_dim_valid_941 : 0000028E6F347020
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1720/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_941() {
  %1(CNode_974) = raise[side_effect_io: Bool(1)]("ValueError", "For 'flatten', 'start_dim' cannot come after 'end_dim'.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1722/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1722/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
}
# Order:
#   1: @check_dim_valid_941:CNode_974{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> For 'flatten', 'start_dim' cannot come after 'end_dim'., [3]: ValueNode<StringImm> None}
#   2: @check_dim_valid_941:CNode_975{[0]: ValueNode<Primitive> Return, [1]: CNode_974}


subgraph attr:
subgraph instance: check_dim_valid_942 : 0000028E6F345AE0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1720/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_942() {
  %1(CNode_977) = call @check_dim_valid_976()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1721/        if start_dim > end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1721/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_942:CNode_977{[0]: ValueNode<FuncGraph> check_dim_valid_976}
#   2: @check_dim_valid_942:CNode_978{[0]: ValueNode<Primitive> Return, [1]: CNode_977}


subgraph attr:
subgraph instance: check_axis_valid_950 : 0000028E6F342B10
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1716/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_950(%para165_axis, %para166_ndim) {
  %1(CNode_979) = S_Prim_negative(%para166_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  %2(CNode_980) = S_Prim_less(%para165_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  %3(CNode_981) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  %4(CNode_982) = Switch(%3, @check_axis_valid_983, @check_axis_valid_984)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  %5(CNode_985) = %4()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  %6(CNode_986) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  %7(CNode_987) = Switch(%6, @check_axis_valid_988, @check_axis_valid_989)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  %8(CNode_990) = %7()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_950:CNode_979{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_950:CNode_980{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_979}
#   3: @check_axis_valid_950:CNode_981{[0]: ValueNode<Primitive> Cond, [1]: CNode_980, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_950:CNode_982{[0]: ValueNode<Primitive> Switch, [1]: CNode_981, [2]: ValueNode<FuncGraph> check_axis_valid_983, [3]: ValueNode<FuncGraph> check_axis_valid_984}
#   5: @check_axis_valid_950:CNode_985{[0]: CNode_982}
#   6: @check_axis_valid_950:CNode_986{[0]: ValueNode<Primitive> Cond, [1]: CNode_985, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_950:CNode_987{[0]: ValueNode<Primitive> Switch, [1]: CNode_986, [2]: ValueNode<FuncGraph> check_axis_valid_988, [3]: ValueNode<FuncGraph> check_axis_valid_989}
#   8: @check_axis_valid_950:CNode_990{[0]: CNode_987}
#   9: @check_axis_valid_950:CNode_991{[0]: ValueNode<Primitive> Return, [1]: CNode_990}


subgraph attr:
subgraph instance: canonicalize_axis_948 : 0000028E6F3425C0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
subgraph @canonicalize_axis_948 parent: [subgraph @canonicalize_axis_916]() {
  Return(%para162_x_rank)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @canonicalize_axis_948:CNode_992{[0]: ValueNode<Primitive> Return, [1]: param_x_rank}


subgraph attr:
subgraph instance: canonicalize_axis_949 : 0000028E6F347570
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
subgraph @canonicalize_axis_949() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @canonicalize_axis_949:CNode_993{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: canonicalize_axis_956 : 0000028E6F346030
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
subgraph @canonicalize_axis_956 parent: [subgraph @canonicalize_axis_916]() {
  Return(%para161_axis)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_956:CNode_994{[0]: ValueNode<Primitive> Return, [1]: param_axis}


subgraph attr:
subgraph instance: canonicalize_axis_957 : 0000028E6F20B3E0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
subgraph @canonicalize_axis_957 parent: [subgraph @canonicalize_axis_916]() {
  %1(CNode_945) = $(canonicalize_axis_916):S_Prim_not_equal(%para162_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_946) = $(canonicalize_axis_916):Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_947) = $(canonicalize_axis_916):Switch(%2, @canonicalize_axis_948, @canonicalize_axis_949)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = $(canonicalize_axis_916):%3()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1725/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_995) = S_Prim_add(%para161_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1727/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_957:CNode_995{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_axis, [2]: ndim}
#   2: @canonicalize_axis_957:CNode_996{[0]: ValueNode<Primitive> Return, [1]: CNode_995}


subgraph attr:
after_block : 1
subgraph instance: flatten_965 : 0000028E6F2049A0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_965 parent: [subgraph @flatten_891]() {
  %1(x_rank) = $(flatten_716):S_Prim_Rank(%para151_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1743/    x_rank = rank_(input)/
  %2(idx) = $(flatten_891):call @canonicalize_axis_916(%para126_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1750/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = $(flatten_891):call @canonicalize_axis_916(%para127_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1751/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_997) = S_Prim_equal(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1757/    if start_dim == end_dim:/
  %5(CNode_998) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1757/    if start_dim == end_dim:/
  %6(CNode_999) = Switch(%5, @flatten_1000, @flatten_1001)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1757/    if start_dim == end_dim:/
  %7(CNode_1002) = %6()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1757/    if start_dim == end_dim:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1757/    if start_dim == end_dim:/
}
# Order:
#   1: @flatten_965:CNode_997{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: idx, [2]: end_dim}
#   2: @flatten_965:CNode_998{[0]: ValueNode<Primitive> Cond, [1]: CNode_997, [2]: ValueNode<BoolImm> false}
#   3: @flatten_965:CNode_999{[0]: ValueNode<Primitive> Switch, [1]: CNode_998, [2]: ValueNode<FuncGraph> flatten_1000, [3]: ValueNode<FuncGraph> flatten_1001}
#   4: @flatten_965:CNode_1002{[0]: CNode_999}
#   5: @flatten_965:CNode_1003{[0]: ValueNode<Primitive> Return, [1]: CNode_1002}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_971 : 0000028E6F11EAC0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_971 parent: [subgraph @_apply_adam_699]() {
  %1(CNode_785) = $(_apply_adam_699):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_adam_opt, S_Prim_Adam[use_locking: Bool(0), use_nesterov: Bool(0), side_effect_mem: Bool(1)], S_Prim_FusedSparseAdam[use_locking: Bool(0), use_nesterov: Bool(0), input_names: ["var", "m", "v", "beta1_power", "beta2_power", "lr", "beta1", "beta2", "epsilon", "grad", "indices"], output_names: ["var", "m", "v"], side_effect_mem: Bool(1), primitive_target: "CPU"], S_Prim_Push[optim_type: "Adam", only_shape_indices: [I64(0), I64(1), I64(2)], output_names: ["key"], side_effect_hidden: Bool(1), primitive_target: "CPU", input_names: ["optim_inputs", "optim_input_shapes"], use_nesterov: Bool(0)], S_Prim_Pull[primitive_target: "CPU", input_names: ["key", "weight"], output_names: ["output"]], Bool(0), Bool(0), Bool(1), %para110_beta1_power, %para111_beta2_power, Tensor(shape=[], dtype=Float32, value=0.9), Tensor(shape=[], dtype=Float32, value=0.999), Tensor(shape=[], dtype=Float32, value=1e-08), %para114_lr)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  %2(success) = $(_apply_adam_699):S_Prim_map(%1, %para115_gradients, %para109_params, %para112_moment1, %para113_moment2, (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)), (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)))
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  Return(%2)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\optim\adam.py:907/        return success/
}
# Order:
#   1: @_apply_adam_971:CNode_1004{[0]: ValueNode<Primitive> Return, [1]: success}


subgraph attr:
after_block : 1
subgraph instance: check_dim_valid_976 : 0000028E6F344AF0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1720/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_976() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @check_dim_valid_976:CNode_1005{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
subgraph instance: check_axis_valid_988 : 0000028E6F341080
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1716/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_988() {
  %1(CNode_1006) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1718/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1718/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @check_axis_valid_988:CNode_1006{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @check_axis_valid_988:CNode_1007{[0]: ValueNode<Primitive> Return, [1]: CNode_1006}


subgraph attr:
subgraph instance: check_axis_valid_989 : 0000028E6F344050
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1716/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_989() {
  %1(CNode_1009) = call @check_axis_valid_1008()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_989:CNode_1009{[0]: ValueNode<FuncGraph> check_axis_valid_1008}
#   2: @check_axis_valid_989:CNode_1010{[0]: ValueNode<Primitive> Return, [1]: CNode_1009}


subgraph attr:
subgraph instance: check_axis_valid_983 : 0000028E6F346AD0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1716/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_983 parent: [subgraph @check_axis_valid_950]() {
  %1(CNode_979) = $(check_axis_valid_950):S_Prim_negative(%para166_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  %2(CNode_980) = $(check_axis_valid_950):S_Prim_less(%para165_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_983:CNode_1011{[0]: ValueNode<Primitive> Return, [1]: CNode_980}


subgraph attr:
subgraph instance: check_axis_valid_984 : 0000028E6F346580
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1716/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_984 parent: [subgraph @check_axis_valid_950]() {
  %1(CNode_1012) = S_Prim_greater_equal(%para165_axis, %para166_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1717/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_984:CNode_1012{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @check_axis_valid_984:CNode_1013{[0]: ValueNode<Primitive> Return, [1]: CNode_1012}


subgraph attr:
subgraph instance: flatten_1000 : 0000028E6F20A940
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1000 parent: [subgraph @flatten_716]() {
  Return(%para151_phi_input)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1758/        return input/
}
# Order:
#   1: @flatten_1000:CNode_1014{[0]: ValueNode<Primitive> Return, [1]: param_phi_input}


subgraph attr:
subgraph instance: flatten_1001 : 0000028E6F207EC0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1001 parent: [subgraph @flatten_891]() {
  %1(CNode_1016) = call @flatten_1015()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1757/    if start_dim == end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1757/    if start_dim == end_dim:/
}
# Order:
#   1: @flatten_1001:CNode_1016{[0]: ValueNode<FuncGraph> flatten_1015}
#   2: @flatten_1001:CNode_1017{[0]: ValueNode<Primitive> Return, [1]: CNode_1016}


subgraph attr:
after_block : 1
subgraph instance: check_axis_valid_1008 : 0000028E6F343B00
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1716/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_1008() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @check_axis_valid_1008:CNode_1018{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: flatten_1015 : 0000028E6F2019D0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1015 parent: [subgraph @flatten_891]() {
  %1(x_rank) = $(flatten_716):S_Prim_Rank(%para151_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1743/    x_rank = rank_(input)/
  %2(idx) = $(flatten_891):call @canonicalize_axis_916(%para126_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1750/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(CNode_1020) = call @flatten_1019(%2, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1762/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_1015:CNode_1021{[0]: ValueNode<Primitive> Return, [1]: CNode_1020}
#   2: @flatten_1015:CNode_1020{[0]: ValueNode<FuncGraph> flatten_1019, [1]: idx, [2]: ValueNode<Int64Imm> 1}


subgraph attr:
is_while_header : 1
subgraph instance: flatten_1019 : 0000028E6F209950
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1019 parent: [subgraph @flatten_891](%para167_, %para168_) {
  %1(x_rank) = $(flatten_716):S_Prim_Rank(%para151_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1743/    x_rank = rank_(input)/
  %2(end_dim) = $(flatten_891):call @canonicalize_axis_916(%para127_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1751/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %3(CNode_1022) = S_Prim_less_equal(%para167_phi_idx, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1762/    while idx <= end_dim:/
  %4(force_while_cond_CNode_1022) = Cond(%3, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1762/    while idx <= end_dim:/
  %5(CNode_1023) = Switch(%4, @flatten_1024, @flatten_1025)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1762/    while idx <= end_dim:/
  %6(CNode_1026) = %5()
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1762/    while idx <= end_dim:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1762/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_1019:CNode_1022{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less_equal, [1]: param_phi_idx, [2]: end_dim}
#   2: @flatten_1019:force_while_cond_CNode_1022{[0]: ValueNode<Primitive> Cond, [1]: CNode_1022, [2]: ValueNode<BoolImm> true}
#   3: @flatten_1019:CNode_1023{[0]: ValueNode<Primitive> Switch, [1]: force_while_cond_CNode_1022, [2]: ValueNode<FuncGraph> flatten_1024, [3]: ValueNode<FuncGraph> flatten_1025}
#   4: @flatten_1019:CNode_1026{[0]: CNode_1023}
#   5: @flatten_1019:CNode_1027{[0]: ValueNode<Primitive> Return, [1]: CNode_1026}


subgraph attr:
subgraph instance: flatten_1024 : 0000028E6F20A3F0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1024 parent: [subgraph @flatten_1019]() {
  %1(idx) = S_Prim_add(%para167_phi_idx, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1764/        idx += 1/
  %2(x_shape) = $(flatten_716):S_Prim_Shape(%para151_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1742/    x_shape = shape_(input)/
  %3(CNode_1028) = S_Prim_getitem(%2, %para167_phi_idx)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1763/        dim_length *= x_shape[idx]/
  %4(dim_length) = S_Prim_mul(%para168_phi_dim_length, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1763/        dim_length *= x_shape[idx]/
  %5(CNode_1029) = call @flatten_1019(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1762/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_1024:CNode_1028{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: param_phi_idx}
#   2: @flatten_1024:dim_length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_phi_dim_length, [2]: CNode_1028}
#   3: @flatten_1024:idx{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_phi_idx, [2]: ValueNode<Int64Imm> 1}
#   4: @flatten_1024:CNode_1030{[0]: ValueNode<Primitive> Return, [1]: CNode_1029}
#   5: @flatten_1024:CNode_1029{[0]: ValueNode<FuncGraph> flatten_1019, [1]: idx, [2]: dim_length}


subgraph attr:
subgraph instance: flatten_1025 : 0000028E6F209EA0
# In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1677/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_1025 parent: [subgraph @flatten_1019]() {
  %1(x_shape) = $(flatten_716):S_Prim_Shape(%para151_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1742/    x_shape = shape_(input)/
  %2(x_rank) = $(flatten_716):S_Prim_Rank(%para151_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1743/    x_rank = rank_(input)/
  %3(idx) = $(flatten_891):call @canonicalize_axis_916(%para126_start_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1750/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %4(CNode_1031) = S_Prim_make_slice(None, %3, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1765/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %5(CNode_1032) = S_Prim_getitem(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1765/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %6(CNode_1033) = S_Prim_MakeTuple(%para168_phi_dim_length)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1765/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %7(CNode_1034) = S_Prim_add(%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1765/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %8(end_dim) = $(flatten_891):call @canonicalize_axis_916(%para127_end_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1751/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %9(CNode_1035) = S_Prim_add(%8, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1765/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %10(CNode_1036) = S_Prim_make_slice(%9, None, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1765/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %11(CNode_1037) = S_Prim_getitem(%1, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1765/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %12(new_shape) = S_Prim_add(%7, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1765/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %13(CNode_1038) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para151_phi_input, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1766/    return reshape_(input, new_shape)/
  Return(%13)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-MyNet/flatten-Flatten)
      # In file D:\anaconda\envs\mindspore_py39\lib\site-packages\mindspore\ops\function\array_func.py:1766/    return reshape_(input, new_shape)/
}
# Order:
#   1: @flatten_1025:CNode_1031{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: idx, [3]: ValueNode<None> None}
#   2: @flatten_1025:CNode_1032{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1031}
#   3: @flatten_1025:CNode_1033{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_phi_dim_length}
#   4: @flatten_1025:CNode_1034{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_1032, [2]: CNode_1033}
#   5: @flatten_1025:CNode_1035{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: end_dim, [2]: ValueNode<Int64Imm> 1}
#   6: @flatten_1025:CNode_1036{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: CNode_1035, [2]: ValueNode<None> None, [3]: ValueNode<None> None}
#   7: @flatten_1025:CNode_1037{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_1036}
#   8: @flatten_1025:new_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_1034, [2]: CNode_1037}
#   9: @flatten_1025:CNode_1038{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: new_shape}
#  10: @flatten_1025:CNode_1039{[0]: ValueNode<Primitive> Return, [1]: CNode_1038}


